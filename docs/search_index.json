[["index.html", "LaBaratory Manual Overview Planned content additions", " LaBaratory Manual Nathan Muncy1 2024-09-03 Overview This digital book contains resources and learning materials compiled for Kevin Labar’s Laboratory of Emotion and Cognition. Content is organized in the following chapters: Chapter 1, Getting Started Chapter 2, Accessing Servers Chapter 3, Psychophysics Chapter 4, Psychtoolbox Chapter 5, Functional MRI Chapter 6, Programming Chapter 7, MySQL Databases Planned content additions Git tutorial Programming in python, R, shell, Matlab Databases NeuroImaging and the Duke Compute Cluster FMRI processing and analyses guide Duke University, nathan.muncy@duke.edu↩︎ "],["getting-started.html", "Chapter 1 Getting Started 1.1 Location 1.2 Duke Card 1.3 Computer Access 1.4 Human Research Training", " Chapter 1 Getting Started Created on 2019 Jun 05 by Gregory Stewart. Ported to bookdown on 2022 Jul 26 by Nathan Muncy. 1.1 Location The CCN is located in the Levine Science Research Center on Research Dr. in Duke’s West campus. It is named after Leon Levine, a NC native that is the founder of the Family Dollar retail stores. The building is a multidisciplinary research center, including computer sciences, environmental sciences, and pharmacology. The building has 4 sections, designated A, B, C, and D wings. Individual rooms are named with the wing prefix followed by 3 digits, the first indicates the floor. For example, the Kevin’s office is located in B247 (B wing and 2nd floor). Doors into the building are unlocked during normal weekday business hours, however they lock in the evening, during weekends, and during holidays. You can enter the building when the doors are locked by using your Duke ID in the external door card swipe on either side of the B wing lobby. The lab is located in B104. The lab is accessed by a door in the main lobby of the building. This door is accessible via Duke ID card swipe or a lab key. There are three testing rooms in the lab. The first is the Main testing room, which includes the Biopac MP160 psychophysiology testing system. The second is Room 2 Behavioral 2 (R2B2), which is the small testing room that can only collect behavioral data. The final testing room, 3rd - Psychophysiology Office (3PO), will contain an MP150 psychophysiology testing setup when the room gets completed. 1.2 Duke Card All students and employees must have a DukeCard, the official ID card for the university. There are 2 types of ID cards, a standard card (Magnetic stripe) and one with an embedded HID chip (Proximity card). All lab members should have cards with a HID chip, security doors at the hospital use these chips to determine security clearance. If you do not have a card or need a replacement, the office is located in the rear of the Telcom Building and there is an exterior door that leads into the office. The office is open M-F from 8am – 6pm. A government issued ID card (e.g., driver’s license, passport, etc.) is required to obtain a DukeCard. The lab will pay for the first card via an IR form, which has to be approved by the department HR representative. Replacement cards (lost/stolen) cost $15. You can also use your DukeCard as a pre-paid debit card through the Flexible Spending Account (FLEX). The FLEX account is automatically created when the DukeCard is issued. There is no minimum deposit to open an account. You can deposit funds into your account online. Employees can also set up an automatic deduction from your pay by going to the Duke@Work My Pay tab. The funds can be used at most places on campus, but you cannot use the account to withdraw cash. If you are a non-Duke student or visiting scholar, the process is similar to the above, but you will also have to get a sponsored Duke account from the LaBar lab manager before beginning the process. 1.3 Computer Access Keoki is the name of our data storage server. The primary folder we use in the lab is experiments2, which contains data from recently completed experiments, current experiments, lab member folders, administrative documents, and website attributes. To get on the whitelist to access Keoki, please see the LaBar lab manager. Once whitelisted, the path to map keoki/experiments2 to your computer is :\\\\ccn-keoki.win.duke.edu\\experiments2 for PC or smb://ccn-keoki.wim.duke.edu/experiments2 for Mac. 1.4 Human Research Training Information outlined below pertains to trainings that must be completed by all key personal part of a DUHS IRB protocol (for a research study are research personnel who are directly involved in conducting the research with human subjects through an interaction or intervention for research purposes, including participating in the consent process by either leading it or contributing to it; OR who are directly involved with recording or processing identifiable private information, including protected health information, related to those subjects for the purpose of conducting the research study): Collaborative Training Initiative (CITI) modules for graduate students and staff: Navigate to www.citiprogram.org Log in via SSO with your Duke NetID and password Choose “Duke Health” Complete required modules: Biomedical Research – Basic/Refresher New individuals are required to take nine modules (automatically populates once appropriate registration is complete). Individuals who have already completed the training will be required to renew their credentials every three years with refresher modules. CITI will send you an automated email 90 days prior to expiration. Undergraduates: Navigate to www.citiprogram.org Log in via SSO with your Duke NetID and password Choose “Duke University Campus IRB” Complete required modules: History and Ethical Principles If an undergraduate is going to be working on a Medical school IRB, they will have to complete the CITI training for grad students/ staff. In addition to CITI, undergraduates who will be involved in the consenting process must also complete Informed Consent Process training specifically for undergraduates. Please note that other conditions and restrictions apply, depending on which department the IRB is housed under; for more information please see the Undergraduates Engaged in Consent Policy. "],["access-server.html", "Chapter 2 Accessing Servers 2.1 LaBar Lab Shared Drive (keoki) 2.2 BIAC Servers", " Chapter 2 Accessing Servers Created on 2020 Aug 25 by Gregory Stewart. Ported to bookdown on 2022 Jul 26 by Nathan Muncy. 2.1 LaBar Lab Shared Drive (keoki) Request LaBar lab whitelist permissions from the lab manager (can sometimes take 24 hours for the update to push through). Map the network drive For Windows, go to My computer or This PC and map network drive. Choose a drive and enter \\\\ccn-keoki.win.duke.edu\\experiments2\\ to access the lab’s shared drive. Keep in mind that this is mapped to the specific PC you are using, so will need to remap it to other computers as needed. For Mac OSx, open Finder and select go. Select connect to server and enter smb://ccn-keoki.win.duke.edu/experiments2/ to access the lab’s shared drive. Keep in mind that this is mapped to the specific PC you are using, so will need to remap it to other computers as needed. 2.2 BIAC Servers This document contains information on installing and setting up the local software necessary to log into and transfer files to/from the BIAC servers. This is required in order to access any imaging data collected at BIAC. Remotely logging into the BIAC servers will also allow you to run scripts on the servers and use the various image analysis software packages installed there. NOTE 1: BIAC recently (11/2015) began requiring Duke’s multi-factor authentication system in order to log into their servers. If you have not set this up already, see this web page for details. NOTE 2: You will need a DHE account to fully log into the BIAC servers. 2.2.1 Mac OSx Users Mac OSX comes with the software necessary to log into and transfer files from the BIAC servers; no additional installation is required. 2.2.1.1 Logging into the BIAC Servers Open a Terminal window. The Terminal application can be found in …/Applications/Utilities. Type ssh -o ServerAliveInterval=120 -o TCPKeepAlive=no -X &lt;username&gt;@hugin.biac.duke.edu (replacing “username” with your actual user name) into the Terminal window. After typing in your password when prompted and completing multi-factor authentication you will be logged into the BIAC servers. If you are presented with an “are you sure you want to connect” question, just type “yes”. 2.2.1.2 Copying Files from the BIAC Servers Open a Terminal window. The Terminal application can be found in …/Applications/Utilities. Use the scp command to copy files between the BIAC servers and your local computer. 2.2.1.3 Examples Copy from BIAC to local: scp &lt;username&gt;@hugin.biac.duke.edu:/path/to/file /localpath/ Copy from local to BIAC: scp &lt;-r&gt; /localpath/ &lt;username&gt;@hugin.biac.duke.edu:/path/to/file In the examples above, replace “username” with your user name, “path” with the full path to the desired directory on the BIAC server, “file” with the name of the file (including 3-character file extension), and “localpath” with the full path to the desired local directory. Note that the scp command has options for copying multiple files at once, for more info on scp, see here. 2.2.2 Windows Users Two additional programs will need to be installed, one that handles logging into the BIAC servers and another that handles file transfer. 2.2.2.1 Logging into the BIAC Servers Download and install X-Win32. This can be done via Duke’s OIT site (Browse and Order Software). Set up X-Win32 to get to the BIAC servers. Follow the instructions here. Once X-Win is set up, select the hugin connection (or whatever you named it) and click “Launch”. Enter your password/multi-factor authenticate when prompted and you should end up with a BIAC server terminal window. If you receive an “are you sure you want to connect” question, just type “yes.” 2.2.2.2 Copying Files from the BIAC Servers Download and install WinSCP. This can be done from Duke’s OIT site (Browse and Order Software). Set up WinSCP to get to the BIAC servers. Create a new “Site” in WinSCP (this can be called whatever you want, but “BIAC” is a good suggestion) with the following values: File Protocol: SFTP Host name: nernst.biac.duke.edu Port number: 22 User Name: (your user name) Once the new site is created, highlight it and click “Login”. After password/authentication prompts this should provide a graphical interface for transferring files, very similar in appearance and function to Windows Explorer. 2.2.3 After Succesful Login The BIAC servers run linux. You’ll need to type linux commands at the command prompt in order to do anything. An introduction to this environment can be found here. The BIAC servers also have a variety of image analysis software packages installed. In order to use any of them, you’ll need to enter “interactive” mode by simply typing “qinteract” into the command line. This may prompt another password/authentication request. The list of installed software can be found here. "],["psychophys.html", "Chapter 3 Psychophysics 3.1 Psychophysics - ECG 3.2 Psychophysics - EDA 3.3 Psychophysics - EGG 3.4 Psychophysics - EMG 3.5 Psychophysics - RSP 3.6 Psychophysics - STIM", " Chapter 3 Psychophysics 3.1 Psychophysics - ECG Created on 2019 Jun 05 by Gregory Stewart. Ported to bookdown on 2022 Jul 27 by Nathan Muncy. 3.1.1 Introduction An electrocardiogram (ECG) is a recording of the electrical activity of the heart using electrodes placed on the skin. These electrodes detect the small electrical changes that are a consequence of cardiac muscle depolarization followed by repolarization during each cardiac cycle (heartbeat)\\(^1\\). Our lab uses a 3 lead setup, where electrodes will be placed at 3 points on the body. Specifically, we follow a lead II configuration (see the diagram below). The lead II configuration consists of having the negative lead attached to the right wrist, the positive lead attached to the left ankle, and a grounded lead attached to the right ankle. We prefer to use this configuration as the electrode placements interfere the least with other psychophysiology measure electrode placements. You can run using other configurations, but keep in mind the possible interference from other electrodes. 3.1.2 Supplies 3 x EL203 disposable electrodes LEAD110 LEAD110S-R (the shielded red lead) LEAD110S-W (the shielded white lead) Paper towel Nuprep 3 x Q-Tips Cotton wipe/ cotton round/ tissue Gel 100 3.1.3 Computer Preparation Log into psychophysiology computer. Turn on BIOPAC system. Open Acknowledge (5.0 for MP160 system, 4.1 for MP150 system). Load experimental file template. Check that the input is Channel 4 (analog). Check where the data is being saved. 3.1.4 Lead Preparation Take the red lead and attach it to the ECG cable in the testing room by attaching the red insert into the Vin\\(+\\) slot and the black insert into the top shield slot. Repeat with the white lead by attaching the white insert into the Vin\\(-\\) slot and the black insert into the bottom shield slot. Insert the black lead with the CBL205 attachment into the GND slot on the ECG cable. 3.1.5 Electrode Preparation and Placement Have the subject wash their hands/wrists with non-moisturizing and dye-free soap then dry with a paper towel (if there is no soap following the specifications, have the participant rinse with just water). Prepare the q-tips with a drop of Nuprep each. Instruct the participant to scrub an area of the lower right wrist/ upper right forearm (see below) with a prepped q-tip (make sure the location is slightly offset to the left of the arm mid-line). Repeat step 3 with the remaining q-tips for the areas behind the participant’s ankles (see below, which is not directly on the ankle, but behind the ankle before the Achilles tendon slightly below the ankle mid-line). Have the participant clean off the Nuprep with a water wet cotton round or tissue. Have participant dry applied areas with another cotton round or tissue. While subject is drying, apply a “pea sized” drop of Gel 100 to the underside of one of the disposable electrodes. Apply the gelled electrode to one of the three sites. Repeat steps 7 and 8 for the two remaining sites. 3.1.6 Lead Application and Clean Up Unlike other psychophysiology measures in the lab, it is absolutely critical to attach and remove the leads to the participant in the correct order! Take black ground lead and attach it to the subject’s right ankle electrode. Take the white lead and attach it to the subject’s right arm electrode. Take the red lead and attach it to the subject’s left ankle electrode. Run the experiment. When finished, remove the leads in the inverse order you applied them (Red lead, white lead, black lead). Have the subjects peel off the electrodes and dispose of them in the trash. Offer the subject a wet paper towel or alcohol wipe to clean up any excess electrode gel that may be on them. Hang all three leads back onto the ECG hook in the testing room. 3.1.7 References Electrocardiography. (2022, Jul 27). In Wikipedia. https://en.wikipedia.org/wiki/Electrocardiography 3.2 Psychophysics - EDA Created on 2019 Jun 05 by Gregory Stewart. Ported to bookdown on 2022 Jul 28 by Nathan Muncy. 3.2.1 Introduction Electrodermal activity (EDA) is the property of the human body that causes continuous variation in the electrical characteristics of the skin. Historically, EDA has been known as galvanic skin response or GSR (the MP150 has a module that is still labelled as GSR. It is believed that skin resistance varies with the state of sweat glands in the skin. Sweating is controlled by the sympathetic nervous system, and skin conductance is an indication of psychological or physiological arousal. If the sympathetic branch of the autonomic nervous system is highly aroused, then sweat gland activity also increases, which in turn increases skin conductance. In this way, skin conductance can be a measure of emotional and sympathetic responses\\(^1\\). Our lab in particular looks at EDA in the palm of the hand, as it allows for subjects to use their fingers to enter computer responses to stimuli, without affecting the data. 3.2.2 Procedures The following outlines the standard procedure for collecting skin conductance data in our lab. Note: Place the electrodes a minimum of 5 minutes before recording, but aim for closer to 15 minutes. Have the subject gently wash their hands with soap and water. Use the specific soap provided by the lab for this purpose, whether in the lab or the BIAC. Moisturizers and anti-bacterial components of soap products can affect conductivity in the skin; these can be difficult to avoid for our purposes, but we can be consistent about our use. More importantly, the soap should not be abrasive (micro-scrubbing beads and such) as this damage to the skin tissue can change its composition and conductivity. Dry hands gently but completely before electrode placement. Use the disposable electrodes that come with gel already applied. Add a pea-sized drop of electrode gel on top of the gel in the electrode. We add fresh gel because the original gel dries over time and may not mix as well with the skin. Place the electrode firmly on the hypothenar eminence of the non-dominant palm at a perpendicular orientation to the eminence leaving space for two adjacent electrodes along the length of the eminence. Apply the second electrode. Apply tape if necessary to keep electrodes in place (for example, if the subject has excessively sweaty hands). While the distal phalanges may be a slightly more ideal site for recording in terms of eccrine sweat gland distribution, the size of the hypothenar eminence allows for easier and more secure electrode placement and allows for more use of the fingers on the recording hand. The subject may make responses with the fingers of the non-dominant hand as long as these responses do not occur during periods that will be analyzed for skin conductance. Further, the subject should be instructed to minimize any unnecessary movement of the recording hand since movement can introduce noise into the signal. Connect the wires to electrodes so they are not obstructive or uncomfortable. Begin acquisition with AcqKnowledge and check for proper signal. You can ask the subject to hold their breath briefly and then exhale to try to prompt a skin conductance response. If it seems likely the subject’s conductance will hit ceiling levels at some point, the gain switch on the front of the GSR BIOPAC module may be repositioned from 10 microsiemens/volt to 20 before beginning the task and the data rescaled in AcqKnowledge accordingly. This should rarely be necessary, but be sure to note this in the subject’s notes and replace the switch after the subject’s session is complete. AcqKnowledge note: We have been recording skin conductance at a frequency of 1 kHz in AcqKnowledge. 3.2.3 References Electrodermal activity. (2022, Jul 28). In Wikipedia. https://en.wikipedia.org/wiki/Electrodermal_activity 3.3 Psychophysics - EGG To be constructed (Jul 28, 2022). 3.4 Psychophysics - EMG Created on 2020 Feb 12 by Gregory Stewart. Ported to bookdown on 2022 Jul 28 by Nathan Muncy. 3.4.1 Introduction An electromyogram (EMG) is a psychophysiology measure for evaluating and recording the electrical activity produced by skeletal muscles. An electromyograph detects the electric potential generated by muscle cells when these cells are electrically or neurologically activated. The LaBar lab most frequently studies Corrugator Facial EMG and Zygomaticus Facial EMG , which focuses on the coruggator supercili group of muscles above the eye and the zygomaticus major muscle along the cheek, respectively\\(^1\\). 3.4.2 Supplies 4 x EL254S reusable shielded electrodes 1 x EL254 reusable un-shielded electrode 5 x ADD204 Stickers Biopac gel 100/ Signa Gel Non moisturizing alcohol wipe Nu-prep Q-tips Tissues Cotton swab/ cotton round 3.4.3 Computer Preparation Log into psychophysiology computer. Check that the BIOPAC EMG module is set to the following: 5000 gain 500 Hz low pass filter 100 Hz high pass filter turned off 10 Hz high pass filter Turn on BIOPAC system. Open Acknowledge (5.0 for MP160 system, 4.1 for MP150 system). Load experimental file template. Check that the sampling rate is 1000 kHz. Check that the input is Channel 2 (for EMG1) or 3 (for EMG2). Check where the data is being saved. 3.4.4 Face Preparation Clean face with an alcohol wipe. OPTIONAL - electrode face preparation can proceed just with the alcohol wipe) Apply the NuPrep (shown in the image above) with a Q-tip to the areas where the electrodes will be applied (see figure below for clarification on electrode placement sites). Apply mild pressure to scrub these areas with the Q-tip. Skin should be slightly red. This step helps to ensure that we are able to remove dead skin cells and have better conductance. If using NuPrep, make sure swabbed area is dry before applying electrodes. Clean the NuPrep off with a cotton round or tissue. Throw away all cotton pads, tissues, and Q-tips used during face prep. 3.4.5 Electrode Preparation Obtain 2 clean shielded EMG electrodes and 1 un-shielded EMG electrodes. The need for an un-shielded electrode is to establish a ground to help reduce noise. If you have a grounded lead already established with either ECG or EDA, you will not need to add this additional ground. Place the circle stickers on the lead so that the hole of the lead align, and so the tab of the sticker is the same direction of the wire as shown in the image below. (Optional for Corrugator) Cut off the top edge and the left edge of the sticker that hang over the edge of the lead for the shielded leads. Place signa gel into electrode well. Using a gel filled syringe, gently fill the well without touching the needle tip to the inside of the electrode. Make sure to fill it to the top of the well, but not too excessively. To get rid of any remaining air bubbles, use the packaging from the alcohol wipe or other rolled scrap of paper to stir the gel. 3.4.6 Electrode Placement - corrugator muscle Place first electrode so that it is in line with the inner corner of the subject’s right eye and just above the subject’s eyebrow. The electrode should be placed so that the wire sits perpendicular to the subject’s eyebrow. Place the second electrode to the left of the first electrode (to the subject’s right). The electrode should be placed so that the wire sits parallel to the subject’s eyebrow. If needed place the third un-shielded electrode in the middle of the forehead close to the hairline. Place the lead wires behind the subject’s right ear and tape them to their right shoulder while the subject looks to their left. This will allow the subject to move or turn without tugging the wires. For the shielded electrodes, plug the black wire (with the green shield marker) into the EMG plugs that say “shield”. Plug the white signal wires into the the ports that say “VIN+” or “VIN-”. Lastly, if you need a ground electrode, plug it into the center hole labelled “GND.” 3.4.7 Electrode Placement - zygomaticus muscle Have the subject smile to see if you can see the general area of the zygomaticus muscle. Place first shielded electrode so that is directly on the muscle, which should be about halfway from the middle of the subject’s ear to the corner of the subject’s mouth. Place the second shielded electrode adjacent to the first in the direction closer to the corner of the subject’s mouth. If needed place the third un-shielded electrode in the middle of the forehead close to the hairline and tuck the electrode lead wire behind the subject’s ear. Tape the electrodes to their right shoulder while the subject looks to their left. This will allow the subject to move or turn without tugging the wires. For the shielded electrodes, plug the black wire (with the green shield marker) into the EMG plugs that say “shield”. Plug the white signal wires into the the ports that say “VIN+” or “VIN-”. Lastly, if you need a ground electrode, plug it into the center hole labelled “GND.” 3.4.8 Cleaning When finished with the experiment, remove the stickers from the electrodes and dispose of them. Use the distilled water squeeze bottle by the sink to clean the gel completely out of the leads. If necessary, soak the electrode heads in a small cup of distilled water for 15 minutes and up to 1 hour and re-rinse with the squeeze bottle. Hang the leads on the rack opposite of the sink to dry completely before using again. Return all the gels, NuPrep, and stickers to their original locations. 3.4.9 References Electromyography. (2022, Jul 27). In Wikipedia. https://en.wikipedia.org/wiki/Electromyography 3.5 Psychophysics - RSP Created on 2020 Sep 04 by Gregory Stewart. Ported to bookdown on 2022 Jul 29 by Nathan Muncy. 3.5.1 Introduction The Respiratory Effort Transducer measures changes in thoracic or abdominal circumference to assess respiratory effort. 3.5.2 Supplies Respiratory transducer belt. Loosen one or both ends of the velcro on the transducer and wrap it around the chest of the subject (or have the subject wrap it around themselves if it is more comfortable). Tighten the Respiratory Effort Transducer around the most expanded area of the subject’s abdomen during breathing (generally about 5 cm below the armpits), and tighten one or both straps until it fits snugly but is not uncomfortable. The sensor should be placed on the front of the chest. Plug the transducer cable into the MEC100C extension cable labelled RSP. If you have trouble getting a signal from the belt, try using one of the back up belts in the drawers of the testing room. 3.6 Psychophysics - STIM Created on 2019 Sep 16 by Gregory Stewart. Ported to bookdown on 2022 Jul 29 by Nathan Muncy. 3.6.1 Supplies 2 x EL203 disposable electrodes 2 x LEAD110 Paper Towel Gel 100 3.6.2 Computer Preparation Log into psychophysiology computer. Turn on BIOPAC system. Turn on the stim box (Note: the stim box sends out a pulse on initializing, so make sure that the subject is not connected). Make sure the stimulation level is set at 0. Open Acknowledge (5.0 for MP160 system, 4.1 for MP150 system). Load experimental file template. Check that the input is Channel 4 (analog). Check where the data is being saved. (Optional) Open Presentation for the stim work-up, ignore if calibrating manually. 3.6.3 Electrode Preparation and Placement Have the subject wash their hands/wrists with non-moisturizing and dye-free soap then dry with a paper towel (if there is no soap following the specifications, have the participant rinse with just water). While subject is drying, apply a “pea sized” drop of Gel 100 to the underside of one of the disposable electrodes. Apply the gelled electrodes the left forearm in close proximity, but not overlapping. Take the two leads attached to the stim cable and connect to the recently placed electrodes. 3.6.4 Stim Calibration Because the tolerance for electrical stimulation can vary wildly by individual subject, we calibrate the voltage level of stimulation each subject receives in a process called “stim work-up.” There are two means of doing the stim work-up; digital and analog. The primary difference is the duration of the stimulation, where you can set the digital to any time length (6 ms usually), whereas the analog is at a fixed 15ms. 3.6.4.1 Digital calibration Open the stim work-up file for the Presentation software (make sure the stim box voltage level is set to 0, or that the subject is disconnected from the electrode leads). Set the voltage level to 10 volts on the stim box. Tell the subject that we are going to tailor their level of stimulation to a point that they find annoying, but not painful and that you will start with a low level starter pulse. Trigger the presentation software to send a pulse (it should be using port code 21). Ask the subject how they feel. Increase the voltage by 10 if the subject feels the level is weak. Repeat steps 4-6 until the subject starts to feel something stronger. Fine tune by increments of 2-5 volts until you find the level where the subject says the stimulation is annoying but not painful. 3.6.4.2 Analog calibration Set the voltage level to 10 volts on the stim box. Tell the subject that we are going to tailor their level of stimulation to a point that they find annoying, but not painful and that you will start with a low level starter pulse. Press the manual pulse button on the back of the stim box. Ask the subject how they feel. Increase the voltage by 10 if the subject feels the level is weak. Repeat steps 4-6 until the subject starts to feel something stronger. Fine tune by increments of 2-5 volts until you find the level where the subject says the stimulation is annoying but not painful. 3.6.5 Clean Up Set the stim box voltage to 0 volts. Turn off the stim box (if the stim box is left on overnight it will likely blow a fuse). Remove the leads from the subject and hang them back up on the stim hook. Instruct the subject to remove the electrodes from their arm and dispose of them. Offer the subjects a wet paper towel or show them the sink to clean excess gel left on their arm. "],["psychtoolbox.html", "Chapter 4 Psychtoolbox 4.1 Introduction 4.2 Questions and Answers 4.3 Screen Function 4.4 Psychtoolboox use at BIAC", " Chapter 4 Psychtoolbox Created by Gregory Steward, last edited on 2019 Jun 05 by Natasha Parikh. Ported to bookdown on 2022 Jul 29 by Nathan Muncy. 4.1 Introduction This wiki page will hopefully be a useful guide to the basics of Psychtoolbox (PTB). For specific questions, please take a look at our Psychtoolbox Q&amp;A section. If you are coming in with little or no MATLAB experience, it may be useful to get an overview of how the language works first, as this guide will assume basic MATLAB understanding. For a quick review of MATLAB and an introduction to PTB, it may be useful to look over the first few weeks of slides provided by Dr. Jonas Kaplan from USC: Intro to Psychtoolbox in Matlab. Below we outline a few basics for working with PTB scripts. Then, we recommend you use the example scripts and demos noted in the second section to see contextual examples of more specific functions. 4.1.1 Basics Screen Function. The Screen command is the basis of PTB scripts, as it controls the way information is displayed to the participant. Because of its fundamental nature, we decided to begin with a brief overview of its functionality so that you can get started on the framework of an experiment. Screen can have many different functions, and many functions work with Screen to create a working program. Before you can present things to the subject, you must first initialize a “window” that will contain the visual material you want to present. You create a new window with the Screen(‘OpenWindow’) function. A simple (but not perfect) way to think about this window is as a double-sided dry erase board. Each side of the board can be referred to as a buffer. At any given time, you present one side to the subject. This is the front buffer. While the subject views the material on the front buffer, you prepare the next screen you want them to see on the back buffer. For example, you can prepare text using the DrawFormattedText() function or images using the Screen(‘DrawTexture’) function. When you are ready to present the next screen, use the Screen(‘Flip’) function to flip the buffers: the back buffer switches to the front and you get a new back buffer to prepare for the next screen and so on. To end your program, you must close the window and anything drawn on it so that you can access your monitor again. To do this, you simply use Screen(‘CloseAll’). We provide a brief example with a detailed description of each function in the Screen Function section. Try-Catch Structure. We recommend using the try-catch structure for your PTB scripts to make them more robust to errors. The next section contains examples of this. The idea here is to embed the bulk of your script (the real “functional” portion) in the try block. MATLAB will attempt to execute everything in the try block per usual. If no errors occur, the catch block will be skipped entirely. If any errors do occur within the try block, MATLAB will immediately jump to the catch block and proceed through whatever commands you have included there to deal with the error situation. This way, an error is much less likely to crash your task in an especially unceremonious way (just freezing, leaving keyboard output disabled, etc.) or go unnoticed until after it is done. Rather, the catch block can be used to properly close files and restore settings as well as report info on the error. 4.1.2 Learn Through Examples One of the best ways to get started is to look through one or more existing PTB experiments and learn how they work. You can then use relevant parts from these as a model for your own PTB script. We are linking two previous, well-commented experiments from the lab below. We believe it’s easier to learn a lot of these functions in context. So, instead of going through all the common functions one by one, we commented these scripts to try to explain what the functions and their components are doing. Experiment 1 Code: Training Session, Main Task This experiment compares different emotion regulation techniques in response to aversive pictures. It involves cuing the subject for a viewing strategy, presenting an image, and presenting several screens asking the subject to make a keyboard response. The full set of files and scripts associated with this experiment can be found in ccn-keoki.win.duke.edu/experiments2/John/EmoReg1. These scripts cover: Using arguments (input you give your script when you call it, such as the subject #). Constructing and using pseudo-randomized stimuli and condition orders. Defining and presenting screens with text or images. Working with keyboard responses. Sending signals to BIOPAC. Writing keyboard responses and trial information to an output data file. Playing audio files (for startle). Triggering screen changes with subject responses vs defined durations. Experiment 2 Code: Monte Carlo This experiment is a three day memory reconsolidation study, including encoding, reactivation with reconsolidation, and retrieval. The goal is to see whether participants can use emotion regulation as a form of reconsolidation. The task itself appears in the form of paired associate learning, where participants are presented with pairs of images, reminded of one, and tested on their memory of the associated picture. This paradigm requires displaying images, cuing regulation strategies, reading in user button presses, and keeping track of old and new images for a memory test. The full set of scripts and stimuli can be found in ccn-keoki.win.duke.edu/experiments2/Natasha/EmoRegCon. In addition to most of the items covered in Experiment 1, this experiment covers: Using mouse clicks to advance screens (practice/training files). Displaying multiple images on a single screen (Session1.m). Online pseudo-randomization in a systematic manner (organizeStims.m) or using a Monte Carlo simulation (randomStimsMC.m, linked to above). Storage and usage of stimuli across multiple scripts (all Session files). Batch scripting (stimCreationBatch.m). BTP Demo Materials There are also a few simple demos that come with the PTB software download itself. On Windows, they are stored in C:\\\\toolbox\\Psychtoolbox\\PsychDemos\\PsychExampleExperiments, and on Mac they are in Applications/Psychtoobox/PsychDemos/PsychExampleExperiments. In fact, the PsychDemos folder has a lot of useful starting information, including demos of how to include video and audio, although sometimes they call functions that are a little dated. 4.2 Questions and Answers Q: Which version of Psychtoolbox should I install? A: The newest version of Psychtoolbox is 64-bit. If your (Windows) computer only supports 32-bit programs or you are planning to use your computer with BIOPAC, you will need to install the 32 bit version of Matlab and Psychtoolbox. This means that your psychtoolbox call will need to include a “flavor.” After downloading the DownloadPsychtoolbox script, open Matlab and use the command DownloadPsychtoolbox(‘Psychtoolbox-3.0.11’) for Windows. Q: I’m getting warnings about GStreamer when I open Matlab. What is the correct way to install this add-on? A: This was a common problem in the lab, and each person dealt with it differently. Here are things that worked for us: If you manually downloaded the 64-bit version from the folders located at gstreamer-org, make sure you downloaded all the files in the newest version, and the folder is located in the same place as your Program Files folder if using Windows (usually in your Local Disk). You may also need to run “SetupPsychtoolbox” or “startup” when you open Matlab. Another fix is to do the automatic download from GStreamer (look for the big green button partway down the page). This site also lets you differentiate between 32-bit and 64-bit versions. Q: I’ve installed the latest version of GStreamer but it causes Matlab to crash as soon as it tries to load a movie. What’s going on? Unfortunately, there seem to be some incompatibilities between the recent releases of GStreamer and some versions of Matlab on Windows. There is a very, very long discussion thread on Yahoo groups here (link broken, NM Jul 29 2022) between several people trying to debug this issue. The recent version of GStreamer (2013.6 Congo) has been verified in the lab to work with Matlab R2010a. It has also been verified to NOT work with R2013b. Some suggestions from the Yahoo thread include using the 32-bit version of GStreamer or using an earlier release of the 64-bit version. None of these have been tested locally. Q: How do you display multiple pictures on a single screen? A: Though PTB documentation suggests using Screen(‘DrawTextures’, …), this call seemed to create a memory error with 3 pictures. Instead, make separate Screen(‘DrawTexture’, …) calls for each picture, where you specify the window in which the picture can be drawn. For example, Screen(‘DrawTexture’, w, tex, [], [0 0 50 50]); will draw your entire picture that is stored in the texture “tex” in a 50x50 pixel square in the top left corner of the screen. Once you have drawn all the textures relating to the pictures you want to show on the single screen, use the flip command to display them all. For more information about the Screen command, see the Basics section and example code in Screen Function section. 4.3 Screen Function In this introduction to the Screen function, we provide some simple code with explanations to orient you to PTB’s structure. Here is an example of the functions mentioned above with supporting functions required to make a working program. [w, wRect] = Screen(&#39;OpenWindow&#39;, 0, BlackIndex(0)); Screen(&#39;TextSize&#39;, w, 32); Screen(&#39;TextFont&#39;, w, &#39;Helvetica&#39;); DrawFormattedText(w, &#39;Hi, there&#39;, &#39;center&#39;, &#39;center&#39;, WhiteIndex(w)); Screen(&#39;Flip&#39;, w); WaitSecs(2.0); DrawFormattedText(w, &#39;How are you?&#39;, &#39;center&#39;, &#39;center&#39;, GrayIndex(w)); Screen(&#39;Flip&#39;, w); WaitSecs(2.0); picture = imread(&#39;smiley.jpg&#39;); tex = Screen(&#39;MakeTexture&#39;, w, picture); Screen(&#39;DrawTexture&#39;, w, tex); Screen(&#39;Flip&#39;, w); WaitSecs(2.0); Screen(&#39;CloseAll&#39;); First, we initialize the double-buffered window we will use for presentation. 0 is the screen number (monitor/display) we want to use for presentation. On a Windows OS, 0 refers to all displays. BlackIndex() is defining the background color for our new window as the standard black value for screen number 0 on the current system. The initialized window is saved as an output (“w” here, referred to as a window pointer). The size of the full window is saved as a 4 component vector, stored in wRect here. Let’s say the vector looks like [0 0 1680 1050]. Each number represents the pixel x or y value of the window, specifically [left (x1), top (y1), right (x2), bottom (y2)]. In our example, (0, 0) would the be the coordinates of the top left corner, and (1680, 1050) would be the coordinates of the bottom right corner. Most of the time, you won’t need to use wRect at all, but we will give an example of where it can be used below. Next are a couple of commands to set the font and font size of text for our new window w. Then, the DrawFormattedText() command tells PTB to write the text ‘Hi, there’ in the vertical and horizontal center of the back buffer (automatically the back buffer) of w in white text. Note, the BlackIndex(), WhiteIndex(), and GrayIndex() functions will accept either a screen number (here is 0) or window pointer (here is w). As soon as the Screen(‘Flip’) command is executed, the screen with ‘Hi, there.’ becomes visible to the viewer. Then, we prepare another message for presentation, ‘How are you?’ in gray text. To keep this screen from replacing the first message after only a handful of milliseconds (the next monitor refresh), we threw in a WaitSecs() command to ensure ‘Hi, there’ is presented for 2 seconds before Screen(‘Flip’) replaces it with ‘How are you?’. Similarly, we show this message for 2 seconds before moving on to the next set of changes to the screen. After the text commands, we have commands that work in conjunction to display a picture on the screen. First, we load the image into a matrix that represents colors by pixel. This process is accomplished by imread(). Next, we prepare the image to be drawn onto the back buffer; think of the Screen(‘MakeTexture’) command as creating a stencil so that drawing the image on the dry erase board will take much less time. Lastly, the picture is drawn onto the back buffer using the Screen(‘DrawTexture’) function. The screen is flipped and the image is displayed for 2 seconds. The Screen(‘DrawTexture’) function can be run with many extra inputs to change the display settings of the picture. After w and the texture input, the fourth input to the function can be a vector specifying how much of the image to display. For example, if you only want to show the top left 50x50 pixel square of the image, you can put in [0 0 50 50] as the fourth input. Similarly, the fifth input specifies where on the screen you want to display the image. This is where wRect comes in useful. Say you only want to show the picture on the top half of the screen. You can input [0 0 wRect(3) wRect(4)/2], where the second y value is divided by 2, implying that this new rectangle only goes halfway down the screen. The last line in our example above is required to properly close the window and anything drawn on it. This call also quits PTB. Generally, you want to call Screen(‘CloseAll’) at the end of your script, but there is also a function called Screen(‘Close’). This call is useful if you have an excessive number of Screen commands in succession (like in a for loop for an event-based paradigm). Like marker residue on a dry erase board, it seems as though PTB doesn’t do a perfect job of erasing the back buffer before drawing the next item onto it. Thus, multiple Screen calls can create a buildup of memory, causing commands to slow down. Adding Screen(‘Close’) at the end of the for loop will properly close all previously drawn objects while keeping the front buffer intact. 4.4 Psychtoolboox use at BIAC Even if you are familiar with Psychtoolbox, getting it to work on the BIAC computers presents a few challenges. Here is a list of some of the things lab members have experienced so that we don’t have to reinvent the wheel every time. 4.4.1 Loading PTB If you try to run your tested PTB code on a BIAC computer without any changes, you will notice that simple PTB commands will fail. For example, AssertOpenGL or a simple OpenWindow using the Screen function may throw errors. It turns out that PTB needs to be set up every time you open MATLAB. You can either do this by navigating to the Psychtoolbox folder on Munin, or add the following two lines of code to your program. If you have many runs that use the same function call, you can also make this a conditional so it only sets up PTB on the first run. addpath(&#39;\\\\Munin\\Data\\Programs\\MATLAB\\PsychToolbox\\3.0.11&#39;); BIACSetupPsychtoolbox; 4.4.2 Triggering Experiment Start Various lab members have had a lot of trouble with this, especially because BIAC5 and BIAC6 seem to work differently. However, code provided to us by Chris Petty seems to work on both scanners. Near the top of your code, you need to make sure you can access and send information to the daq (data acquisition device. To do that, put in the following lines: try daq = DaqDeviceIndex(); catch error(&#39;Daq device not found&#39;); end Then, right before the start of your experiment, you can paste the following code. This will take the number that the counter is at (which counts the number of TTL pulses) and wait for another to be sent out before continuing. Note that there is a pulse per slice acquisition. curcount = DaqCIn(daq); while 1 if DaqCIn(daq) &gt; curcount % start your task break else pause(.05) % do short sleep here just so you’re not executing % the counter check a billion times end end 4.4.3 Recording Events Onset and Duration To do fMRI analysis, it is important to know the onset time and duration of each event that occurs in your task. There are a few easy ways to do this with PTB. Firstly, if you want to manually get the time whenever you want, you can use the command GetSecs; to retrieve the current computer time. You should do this near the beginning of your task and after your disdaqs to get a sense of your start time; this start time can be subtracted out of all other timing calls in the future to get a relative time (in terms of start of experiment/scan) instead of the absolute computer time. You can also use timing output from the command Screen(‘Flip’). There are three timing outputs from this command; the second is the one that is closest to stimulus presentation. So, using the code [~ onsetTime] = Screen('Flip'); will give you the time at which the stimulus shown on the next screen is displayed (in the same format as GetSecs, so the absolute computer time). You can use this to get specific onsets of stimulus presentations. Note that both of these will be in milliseconds, so divide the output by 1000 for seconds. Similarly, you can use either of these two methods to get durations – simply get the timing after an event goes away (using either method) and subtract the onset time to get the duration. Both onset and duration are important for fMRI analysis. 4.4.4 Using the Button Box If you’re using the button box(es) for responses in the scanner, it is useful to know that the buttons do not linearly increase from left to right. If you’re only using one box, specifically, the right one, you should be fine, but the problem comes when you try to use both. Here is the coding for the buttons, from left to right. Left hand: 9 8 7 6, Right hand: 1 2 3 4 Also, the buttons register as if they are the numbers on the top of the keyboard, so all of them will have a KbName of 1!, 2@, etc. Also, the button presses are extra sticky in the scanner. This isn’t a problem usually when you’re looking for a response in a certain time window, but it can be an issue if you have responses advancing your screens. Add in extra wait times after those. Other thing of note: the projected screen into the scanner is quite small, pixelwise, so try to leave extra space around your words/images when testing elsewhere. Hopefully that proved at least a little helpful! Good luck! "],["fmri.html", "Chapter 5 Functional MRI 5.1 Introduction 5.2 Initiating and Running an MRI Study at Duke 5.3 SPM 5.4 QA - FMRIPrep", " Chapter 5 Functional MRI 5.1 Introduction Created on 2019 May 30 by Gregory Stewart. Ported to bookdown on 2022 Aug 01 by Nathan Muncy. One of the primary types of data the LaBaratory collects is functional magnetic resonance imaging (fMRI) data. In particular, we analyze the changes in blood oxygen level of the brain to different tasks and stimuli (in what is called blood oxygen level dependent fMRI or BOLD fMRI). Some basic information about the method and its history can be found here. Educational Resources: Dr. Geoff Aguirre (UPenn Neurology) regularly teaches introductory seminars on MRI and fMRI. This is one of his more concise presentations (~90 minutes in total). Another presentation from Dr. Aguirre that goes into more depth (~7 hours in total). There is an annual 2-week course on fMRI at U Michigan, which is targeted at researchers with little-to-no background in MRI, and they have posted all of their materials. These lectures are very long and detailed, but broken down by specific topics (many lectures of varying lengths). Lab members may also refer to one of the editions of the textbook Functional Magnetic Resonance Imaging (Huettel, Song, &amp; McCarthy) in lab. A useful reference, particularly for MR physics. Blogs with lots of useful information on various aspects of fMRI including artifacts, analyses, and software tools: https://practicalfmri.blogspot.com/ http://andysbrainblog.blogspot.com/ 5.2 Initiating and Running an MRI Study at Duke Created on 2018 Feb 21 by John Powers. Ported to bookdown on 2022 Aug 01 by Nathan Muncy. 5.2.1 IRB Protocol Approval To begin an MRI experiment at Duke, you are going to need to get your experiment approved by 2 entities, each with their own procedures and rules. They are the medical center IRB and the Brain Imaging and Analysis Center (BIAC). First, you need to complete your CITI training /Human Subject Research Training. If you need to complete the training, go to citiprogram.org. Log in with your institution (using your Duke information). When prompted, join the Duke Health portal and complete their courses: Vulnerable Subjects – Prisoners Vulnerable Subjects – Children Vulnerable Subjects – Pregnant Women, Biomedical Research CITI Good clinical practice. It should take a few hours and you need an 80% passing rate to get certification. Once completed, email eIRB@mc.duke.edu to establish a Medical Center IRB account. Once logged into https://eirb.mc.duke.edu, start a new IRB application with the “New Study Application” button. Once there, fill out the requested information. Remember to fill out the information for the psychiatry department as they are the hospital department that handles the MRI studies that deal with our experimental subject matters (you could also choose radiology as an alternative). It is also recommended at this time to make an appointment with the representatives at BIAC to discuss their recommendations for the MRI protocols for your experiment. Once you fill out the information (with the assistance given by BIAC), attach all relevant documents (like the summary of the research protocol, the informed consent form you plan on using, and any recruitment materials you may use) and submit the experiment for review. The process of undergoing review is time-consuming. You will likely meet with the Clinical Research Unit to discuss your IRB and make edits. You will also have to edit or clarify the protocol as deemed by the department reviewing the protocol. 5.2.2 BIAC Safety Certification Once your medical IRB has been established, you can request door access and computer access at the BIAC scanners from the BIAC help desk: biac-help@duke.edu. They will provide you with the materials you need to begin scanning. In particular, you will need to be MRI safety certified. The information about the certification can be found here: http://www.biac.duke.edu/research/safety/. You will need to complete the safety tutorial and complete the safety quiz. 5.2.3 BIAC Protocol Approval Once completed, go to the “getting started” page http://www.biac.duke.edu/research/gettingstarted.asp and follow their instructions on how to complete the BIAC research proposal (it will be a more condensed version of the information you filled out for the IRB). This process should go quickly as you should have met with the people at BIAC before you submitted your IRB. You will need to include information such as study personnel, which scanners you will need, how many hours you want for scanning, and experimental summary. The BIAC scientific review committee meets once near the beginning of each month to review submissions. 5.2.4 Using the BIAC Calendar To access the scanner calendar, go to the BIAC website www.biac.duke.edu and go to their services page and select the link to the calendar. Alternatively, you can however over the services tab on the homepage and choose calendar. You will need to log in with your Duke Health Enterprise (DHE) account (established when BIAC set up your computer access; will match Duke netID and password). The calendar acts much like a typical web calendar (e.g. outlook, google, etc.) To reserve a slot, choose the room in the scanner field on the left. Typically, you will only be scanning in either BIAC5 or BIAC6 and possibly using the TEST1 and MOCK1 rooms for practice. Once you selected the appropriate room, choose the time you wish to reserve the room. You will be redirected to a study details page to fill out. Choose the start time, the end time, the experimenter (you), and the experimental protocol you are running. Once everything is selected, click the add button and your time will be allotted. Also, keep in mind the availability of the scanner techs. To check their availability, click on the MR Tech and Pre-Sold Slots Schedule link on the calendar main page. This will redirect you to the scanner techs’ schedules. If one of the techs is unavailable during their normal hours, there will be a note on the header of the scanner page for that day. You should be able to book the time you want as long as more than 1 tech is available. On the off chance that there is only 1 tech available, check and see if the other scanner is free. If it is free, book a scan for that room and choose the Null.01 option for experiment. On your scanner page, book the scan using the normal procedures. When booking a time slot, you will notice an option for UserTest.01 in the experiment section. User tests are a way for you to book the scanner room to test out your procedures without having to pay for the time (more information in “BIAC Study Preparation” below). TEST1 and MOCK1 are additional spaces that can be reserved to run your study. MOCK1 is equipped with a non-functioning MRI scanner and a computer that contains a few typical scanner noises. If you have a subject who has never been in an MRI scan before, it might be good practice to take them there and get them use to the scanning environment/scanner noises before the experiment. TEST1 is a small behavioral testing room. Booking and canceling either of these rooms is free of charge, but you must request hours for these rooms in your BIAC research proposal and have them approved (this will generate calendar protocols for your study that you can use to schedule on their calendars). See “BIAC Study Preparation” below for uses for these rooms. If you cancel a scanner time slot more than 1 day before the scan, you can cancel it and remove it from the calendar, free of charge. If you need to cancel the study the day before or the day of the scan, the funding source for the study will still be charged partially for the scan. You will not be charged anything for cancelling MOCK1 or TEST1 bookings. After a calendar event occurs, you will need to complete the calendar entry (the slot will turn orange when you need to complete it). You will have to add the exam number (the scan number), tech information, the type of consent form signed (if you have more than 1), subject information, the amount the subject was compensated, whether there were any problems with the scan, and a pregnancy test results (if applicable). When you fill out and save all of this information, the calendar entry will return to the standard blue color. If you would like to see some information about your experiments, click the “Experiment Info Page” on the left. You will be redirected to a page with all of your experiments. You might notice that there will be some repeat scans with different numbers as endings. This is to distinguish which hours you use in which room. For example, an experiment ending in .01 might be for BIAC6 and .61 might be for TEST1. When you click on one of the experiments, it will give you information about the study, including but not limited to the study personnel, the number of hours you have allocated, and calendar entries you need to complete. If you notice you are running low on hours, you can request more in an email to the BIAC help desk biac-help@duke.edu. You can also change the number of hours requested when your BIAC protocol goes through its continuing review process a year after initial approval. 5.2.5 BIAC Study Preparation Once BIAC approves your protocol, they should create directories for your study on a BIAC server Munin3/LaBar and the scratch drives of the BIAC room computers (their local drives), and you should be given access to these locations. If this is not done, you can contact biac-help@duke.edu. To get your study files onto the scanner control room computers, you can upload them to your directory on Munin3 and then download them at the scanner PCs into your scratch drive directories. Information on remotely accessing the BIAC servers can be found here: Windows Mac For Windows, the software WinSCP is a reliable SFTP client option. You will need to mount the Munin3 server onto the scanner room PCs the first time you access them. Generally, behavioral and psychophysiological data are transferred from these PCs to the server after each session, and then later downloaded to keoki. The left workstation in each control room is for the stimulus presentation computer (has two monitors) and the right is for the BIOPAC computer. Investigators listed on an approved imaging protocol should be able to request DukeCard access to the scanning facilities by contacting biac-help@duke.edu. You must have a DukeCard with an embedded RFID chip for this to work (not the default DukeCard type). If you need to obtain this type of DukeCard, talk to Greg to get the appropriate form (stating the lab is covering the cost of the card), and then take it to the DukeCard Office to request the new card. Once you have calendar access, you will need to schedule UserTests to set up and test your scanning protocol on the scanner. User tests can be scheduled during empty time slots within 24 hours of the slot (essentially claiming what would have been unused scanner time) and are free. Communicate with the techs either at this point or before this point to construct a scan protocol for the study. You should request a tech’s presence in the “Notes” section of the UserTest calendar entry. Things to check during UserTests include: stimulus presentation works on the scanner PC; length of task matches the functional scan length; communication between scanner, presentation PC, and BIOPAC PC is working as intended; stimulus software correctly saves necessary behavioral data; behavioral and physiological data can be transferred to the BIAC server for retrieval later; etc. Note that BIAC policy does not allow you to receive the MRI data from UserTests. If there is a specific reason why you would need to see this data to develop your protocol, you can request access to the data from Todd Harshbarger (todd.harshbarger@duke.edu) and Chris Petty (chris.petty@duke.edu). BIAC has a mock scanner room (MOCK1) that you can reserve if needed. In addition to having a mock MRI scanner, this room has a PC that is comparable to the stimulus control computers in the scanner control rooms. It can be used to test that your tasks and stimuli will present successfully. This room or TEST1 (same restrictions apply) can also be reserved for paperwork, behavioral testing, and other procedures before or after scanning sessions. 5.2.6 Study Running ou must bring a BIAC safety screening form to each session, and the subject must complete this form the day of their scan. The screening form can be found here. Note that subjects in the BIAC subject pool have been pre-screened, so you can just do a screening with them the day of their scan. For subjects recruited outside of the BIAC subject pool, you are expected to pre-screen them for MRI safety before scheduling a session for them on the calendar. Have subjects place all belongings not going into the scanner with them into the lockers near the scanner hallway entrance. Female subjects of child-bearing potential will be required to complete a pregnancy test before entering the scanner (see age guidelines below). Before the session start time, the experimenter should obtain a urine screen kit from the scanner control room. The kit should consist of the insulated bag containing a zip-top bag contained a packaged sample cup. When entering the control room for the session, place the insulated bag with the urine sample on the papered table, and the tech will run the pregnancy test. Age 55 or older – no test required Age 50-54 – no test required if most recent period was more than 12 months ago Age 45-49 – no test required if most recent period was more than 18 months ago Remember to move data files to a secure location at the end of the session (usually to Munin3 server). Our space on Munin3 is very limited, so after BIAC QA (quality assurance) processing is completed, move files off Munin3 to keoki. Even if the BIAC subject pool coordinator schedules a session, the researcher is responsible for completing calendar entries after the session (sometimes the techs will complete the scanner calendar entries; see “Using the BIAC Calendar” above for more information). Entries for MOCK1 and TEST1 calendars must be completed as well (sometimes this just involves opening them and saving them). 5.3 SPM Created by Greg Stewart, last modified on 2018 Feb 18 by Natasha Parikh. Ported to bookdown on 2022 Aug 01 by Nathan Muncy. Installing and Using SPM Preprocessing Batch Script Templates This document contains instructions for installing and using the SPM fMRI preprocessing template batch scripts. These scripts are designed to perform initial preprocessing on an fMRI image and a matched T1 anatomical image. The two images should be roughly in alignment before putting them into this preprocessing (if they were acquired during the same scan session the initial alignment should be okay). This preprocessing will produce a version of the fMRI data and a version of the T1 data spatially normalized to MNI-152 standard space ready to go into first-level analysis. These scripts will also produce a quality assurance (QA) HTML file for initial evaluation of the fMRI data. The general flow of the preprocessing is as follows: install the necessary scripts and SPM modules, edit the template batch script to match the specifics of the data at hand, run one subject to make sure the SPM batch works, edit the template multi-subject processing script to match the specifics of the data at hand, run the multi-subject processing script. NOTE: You’ll need to be able to log into and transfer files to/from the BIAC servers in order to have any access to your imaging data. If you have not set this up on your local computer, instructions for doing so can be found on the wiki here. 5.3.1 Basic File Manipulation Terms and Information “Unzip”: The zip files associated with the batch script templates all contain directories of Matlab scripts and SPM modules. To unzip each of them, right click on the .zip file to be unzipped; select 7-Zip -&gt; Extract Here. This will create an unzipped version of the directory in the same directory in which the .zip file is sitting. “Add to Matlab path”: The “Matlab path” is simply a list of directories that Matlab will search to find functions the user attempts to call. In order for a new function or script to be run, it must either be saved into a directory that is already in this list, or the directory in which it resides must be added to the list. In most cases the cleanest way to handle new modules and scripts is the second option: keep them in a new directory and add that directory to the Matlab path. In the main Matlab window, the current directory is displayed along the top, just under the grey panel, in the thin white bar. The contents of this directory are displayed along the left-hand side of the window. The current directory can be changed by either double-clicking on directories in the left-hand panel or by clicking on parts of the directory path in the white bar. To add a directory to the Matlab path, right click on it in the left-hand panel and select Add to Path -&gt; Selected Folders and Subfolders. This will add the directory to the Matlab path until Matlab is closed. When re-opening Matlab the directory will need to be added to the path again. Changes to the path can be saved between Matlab sessions using the Set Path button to bring up the path editing window and pushing the Save button or typing “savepath” into the Matlab command line. Either method will require administrative access on the computer for changes to actually be saved. “SPM8 Install Directory”: The SPM install directory is the directory containing the actual spm.m script file that gets called when you type spm into the Matlab command line. It also contains sub-directories housing various SPM toolboxes and modules. Typing which spm into the Matlab command line should display the location of this directory (if instead it produces an error, odds are the SPM install directory has not been added to your Matlab path). 5.3.2 Installing Necessary Script Files All the parts necessary to use the SPM fMRI Preprocessing Batch Templates are saved on Keoki in: …/Graner/Data/SPM_Preprocessing_Things Copy Graner_Batch_Modules.zip to your local drive and unzip it. The unzipped directory contains Matlab scripts for implementing a quality-assurance module in SPM. Move the unzipped directory to a place on your hard drive in which you want such scripts to reside (if nowhere else comes to mind, this can go in the MATLAB directory). Add, with subfolders, the Graner_Batch_Modules directory to your Matlab path. Copy SPM_Batch_Templates.zip to your local drive and unzip it. This directory contains templates of SPM batch scripts for running preprocessing of fMRI data. The directory should be moved somewhere near wherever you are planning on saving the output results (this is just a “good practice” suggestion; the directory can actually go anywhere, but it is often helpful to have analysis scripts near their associated outputs to keep a clear data/analysis trail). Copy vbm8.zip to your local drive and unzip it. The unzipped directory contains the VBM8 module for SPM created by Christian Gaser (http://www.neuro.uni-jena.de/vbm/). It will be used to segment the anatomical T1 image and does a better job than the built-in SPM module. This directory must go in a specific place. Move it into the directory that resides in the SPM8 install directory. (After the unzip process make sure there is only a single vbm8 directory rather than a pair of nested vbm8 directories.) After moving the vbm8 directory into \\toolbox, you will need to either restart SPM or add, with subfolders, the vbm8 directory to your Matlab path. 5.3.3 Checking SPM Module Installation Open Matlab, add the Graner_Batch_Modules directory to the Matlab path, and type spm fmri into the command line. Click the “Batch” button on the bottom of the SPM window. The drop-down menus across the top of the Batch Editor window should include one called LocalMods with QA and ReadHdr sub-options. If this menu is not there the Graner_Batch_Modules directory may not have been added to the Matlab path. There may also have been an error when trying to load the modules. If this were the case there should be some red error text in the main Matlab window. The VBM8 module should show up in the SPM -&gt; Tools dropdown menu. If “VBM8” is not an option in this sub-menu the vbm8 directory may not be in the correct place. If the checks above all went smoothly, select the File -&gt; Load Batch option in the batch editor window. Navigate to the SPM_batch_templates\\SPM8\\ directory, select fmri_batch_job.m and click Done. If everything is good-to-go the following list of modules should be listed in the left-hand portion of the window in this order: 3D to 4D File Conversion Expand image frames Slice Timing Realign: Estimate &amp; Reslice VBM8: Estimate &amp; Write Image Calculator Image Calculator Coregister: Estimate Deformations Deformations Smooth QArunall Move/Delete Files 5.3.4 FMRI preprocessing steps Created on 2018 Feb 13 by Gregory Stewart. Ported to bookdown on 2022 Aug 01 by Nathan Muncy. Summary of fMRI Preprocessing Steps in the SPM Template Scripts This is a list of descriptions of the various preprocessing steps included in the SPM fMRI preprocessing template batch scripts. They are presented in the order in which they are applied in the SPM template scripts. Selection of Image Volumes to Preprocess (“3D to 4D File Conversion” and “Expand Image Frames”). These two modules are in place to allow the script to easily handle input data that are in the form of a single 4D file or many separate 3D files. While their main function is mostly to make sure the volume list passed to the Slice Timing module is in the correct format, this is also where any pre-steady-state(*) image volumes can be excluded from the preprocessing. Note: pre-steady-state volumes (a.k.a. “disdaqs”) may have already been excluded from the data before being exported from the scanner. Slice-Time Correction (“Slice Timing”). This module adjusts the fMRI signal of every slice in order to correct for the fact that not all slices were actually acquired at the same time. Between the first slice acquisition and the last slice acquisition of each volume, almost an entire TR (usually 2 seconds) has passed. This time difference is not insignificant given the temporal scale of the hemodynamic response. Slice-Time correction interpolates the signal of each slice to the timing of a desired reference slice in order to account for this temporal discrepancy. Motion Correction (“Realign: Estimate &amp; Reslice”). This module spatially realigns each volume of the fMRI series to a reference volume using a 6-parameter affine transformation (rotation and translation about and in the x, y, and z axes). This is meant to correct for movement of the participant’s head in the scanner during acquisition, so that each image voxel represents the same 3D chunk of tissue throughout all time-points. Segmentation of T1 and Calculation of MNI152-Space Transforms (“VBM8: Estimate &amp; Write”). This is a (well-known) third-party module for SPM that creates white matter, grey matter, and cerebral spinal fluid masks from the T1 image. In the process, it moves the T1 to standard MNI-152 space using a non-linear, diffeomorphic warp. Although the MNI-space version of the T1 is not saved, the module does write out the transform data necessary to carry out this transformation. Segmentation masks are stored in the native T1 space (i.e. not MNI). Creation of a Brain Mask from the T1 Data (“Image Calculator”). In general, the Image Calculator module can be used to carry out mathematical operations on image data. In this case, it is used to piece together the three segmentation masks created by VBM8 into a full-brain mask (just by adding the three segmentation masks together). Skull-Stripping the T1 Image (“Image Calculator”). This instance of Image Calculator multiplies the T1 image by a binarized (voxels equal to 1 if they are in the mask, 0 outside the mask) version of the brain mask created in the previous Image Calculator. The resulting image only contains the brain. Register the fMRI to the T1 (“Coregister: Estimate”). This module spatially registers the fMRI image to the T1 using an affine transformation. In general, the two images should be very close to each other before this step. Coregistration is meant to account for any participant head movement between the acquisition of the T1 and the acquisition of the fMRI data. Move the T1 to Standard MNI-152 Space (“Deformations”). This module applies the deformation field created by the VBM8 module to the skull-stripped T1, creating a version of the T1 in standard MNI-152 space. Move the fMRI Data to Standard MNI-152 Space (“Deformations”). As with the T1, this module applies the deformation fields created by VBM8 to the fMRI data. Because the fMRI data have been coregistered to the T1, the deformation field created for the T1 can be used on the fMRI data. This module also uses a pre-existing 3D template image to write the transformed fMRI data into a grid space with voxel sizes similar to those of the original fMRI data rather than the T1 data. T1 voxels sizes are much smaller. fMRI data files resampled to a T1 grid-space are very, very large. Spatial Smoothing of the fMRI Data (“Smooth”). This module applies a 3D spatial smoothing kernel to each fMRI volume. The template uses a default kernel full-width at half-max. (FWHM) of 6mm in each direction. Smoothing of fMRI data is generally done in an attempt to “average out” some of the random noise while maintaining neural activity-related signal. Create Quality Assurance Pictures and Metrics (“QArunall”). This is a module I wrote to quickly and automatically create some initial quality assurance (QA) information on the T1 and fMRI images. It is meant to provide a look at the general quality of the image data. More information about the output from this module can be found in the documentation file, QA_output_notes.docx. Clean Up (“Move/Delete Files”). This module deletes several intermediate image files once everything has run, which can save a lot of hard drive space in the long-run. Before the first excitation pulse the longitudinal (in line with the main magnetic field) component of the hydrogen nuclei group magnetization is at a maximum. After the first pulse the longitudinal component is decreased and begins recovering at an exponential rate based on the degree to which it was decreased (The excitation pulse knocks a portion of the hydrogen nuclei into a high-energy state in which their magnetic moments are anti-aligned to the main magnetic field. These then randomly return to the initial lower-energy state. The rate at which hydrogen nuclei are returning to the lower-energy state in the whole sample is thus dependent on the number of hydrogen nuclei in the excited state (dx = a\\(\\times\\)x), leading to an exponential decay in the whole sample’s longitudinal magnetization.). In steady-state, the longitudinal magnetization is “in synch” with the excitation pulses such that it always returns to the same value before the next pulse arrives. Before this happens, the fMRI signal will be very different between volumes. 5.3.5 Reorganizing Image Data and Copying Them from the BIAC Servers In order for the multi-subject processing script to work properly, the subjects’ data for a given study must be inside a directory structure named such that the path to each subject’s fMRI and T1 image files is distinguished from that of other subjects only by that subject’s unique study ID. In other words, if one replaced all occurrences of the subject ID in a subject’s data path with the subject ID of another subject, the new path would be a valid path to the second subject’s data. An example of such a directory structure is: …\\Data\\Study1\\Subjects\\0001\\fmri\\run_01_0001.nii ...\\Data\\Study1\\Subjects\\0001\\anat\\t1_0001.nii ...\\Data\\Study1\\Subjects\\0002\\fmri\\run_01_0002.nii ...\\Data\\Study1\\Subjects\\0002\\anat\\t1_0002.nii In this example, replacing all occurrences of the first subject’s subject ID (“0001”) in that subject’s file paths and names produces the paths and names of the second subject’s (“0002”) files. When data are initially saved on the BIAC server, they will be in a slightly different directory structure. However, the biac_create_preproc_dir.py Python script packaged in Graner_Batch_Modules.zip will create the directory structure required by the multi-subject processing script and copy the imaging data into it. Create a copy of the biac_create_preproc_dir.py script in the BIAC study directory containing all of your imaging data (the directory on the BIAC server that also contains the directory called “Data”). This can be done by copying the script over to the BIAC server from your local computer. Log into the BIAC server and enter interactive mode (see the wiki page here). Navigate to the study directory and type python –m biac_create_preproc_dir.py into the command line. If anything goes wrong with the script it should print a message to the terminal window. The script will create a log file that starts with biac_create_preproc_dir_log and ends in a long time-stamp and .txt extension. This log file should be written to the study directory and will contain much more output about what the script did. Open this log file (type nedit ./filename into the terminal, where filename is replaced by the name of the log file) and look for any WARNINGs. If there are, some trouble-shooting may be necessary. If all else fails, ask John G. about it. The default version of the script has a flag set so that it does not actually copy anything, but only writes to the log file what it WOULD HAVE done. This is so you can check to make sure it will do what it is supposed to and prevent anything dire from occurring. If the log file from part 3 looks good, edit the script so that it actually makes copies of the data. Type nedit ./biac_create_preproc_dir.py, find the line (probably around line #36) that reads actually_copy = 0, change the 0 to a 1, save the file, and close it. Now rerun the file using python –m biac_create_preproc_dir.py. Again, look for any WARNINGs in the new log file. Look for a new Preprocessing directory in the study directory. This new directory should contain one sub-directory per subject, containing all of that subject’s imaging data. Briefly look through some of these sub-directories to make sure it looks like this is the case. If everything looks good in the new Preprocessing directory, prepare to copy it and all of its sub-directories to your local computer. First, make sure there is enough room on your local computer to hold a copy of the data: type du –h ./Preprocessing on the command line. The first number that appears in the output from this command is the total disk space used by the Preprocessing directory. Check your local drive to make sure there is at least this much space plus 1 GB free. If not, see if there is anything you can clean out… Copy the Preprocessing directory to your local computer using either WinSCP (Windows) or the scp command with the “-r” option in a new terminal window (OSX). You can now carry out preprocessing/analysis on the local copy of the data. If something goes wrong and the local copy gets partially deleted or otherwise compromised, you can simply re-copy the data from the BIAC server. 5.3.6 Image Parameters and Image Format Necessary for Preprocessing The image files to be processed must be in NIFTI format. The functional data may be a single 4D NIFTI file or a series of 3D NIFTI files. The anatomical T1 data must be a single 3D NIFTI file. The SPM preprocessing batch templates have many processing parameters set already but also require some information specific to the images to be processed. The following is a list of parameters you will need to enter into the template and information on how to extract them from images in NIFTI format (NOTE: The “private” field items listed below are not necessarily standard and may be missing or named differently. If they are absent, you may need to go back to the BIAC header file (.bxh) or, as a last resort, ask the MRI technologist who acquired the images about these parameters). You may already know what these are without needing to get them from the headers (but it never hurts to double-check). In order to read the necessary information from the files, you’ll first need to load the headers into Matlab using some SPM functions. These functions are called from the Matlab command line (not via the SPM graphical interface). Navigate, in Matlab, to the directory containing the functional image to be processed. Enter vol = spm_vol(FILENAME) into the Matlab command line, with FILENAME replaced by the name of the NIFTI functional image file. This will load the image header information into the variable “vol.” Slice Acquisition Order. Type vol(1).private.diminfo.slice_time into the Matlab command line. This should list four fields related to the order and timing of slice acquisition for the functional sequence. The “code” field should contain a string describing how the slices were acquired. It will most likely include either “alternating” or “sequential” and either “increasing” or “decreasing”. The first pair of words refers to the general order in which the slices were acquired. “Alternating” means half the slices of the brain were acquired in an “every-other” fashion (i.e. slice 1, then slice3, then slice 5, etc.) until the edge of the image was reached and then the other half were acquired, again in an every-other fashion. “Sequential” means all the slices were simply acquired in order across the image (NOTE: the word “order” in this sentence and the slice numbers in the parenthetical phrase of the previous sentence are referencing the spatial order of the slices, NOT the temporal order). The second pair of words refers to which slice was acquired first. “Increasing” means slice 1 was acquired first. “Decreasing” means slice N was acquired first, where N is the total number of slices in the image. Thus, if an image had 6 slices and was acquired alternating_increasing the slice order would be [1, 3, 5, 2, 4, 6]. If the same image had been acquired sequential_decreasing the slice order would be [6, 5, 4, 3, 2, 1]. To add to the fun, different imaging centers or different scanners may use different words for these slice acquisition patterns. For example, alternating_increasing may be referred to as odd_up or similar. The “start” and “end” fields simply contain the first and last slices acquired. The “duration” field contains the acquisition time of each slice, in seconds. This multiplied by the number of slices should equal the acquisition TR. Number of Slices. Type vol(1).dim(3) into the Matlab command line. The displayed number should be the number of slices in the image. As a double-check, type vol(1).dims into the command line. This will display all three dimension sizes of the image. The number of slices will almost always be the one number that is smaller and different than the other two. TR. Type vol(1).private.timing.tspace into the Matlab command line. The displayed number should be the TR of the image acquisition, in seconds. TA. This is actually a value calculated from the TR and the number of slices and is equal to \\(TA = TR - \\frac{TR}{\\#slices}\\). The help box in the Slice Timing SPM module (where you’ll need to enter the TA) also has this equation listed. Number of Initial Image Volumes Discarded from Functional Image (“disdaqs”). During the first few volume acquisitions the fMRI BOLD signal will change rapidly because the longitudinal component of the collective magnetic moment of the hydrogen atoms within each voxel has not yet reached a steady state. Some imaging data have already had these volumes discarded while others have not. You will need to know if the functional data to be processed still contain these initial volumes or if they have been removed. 5.3.7 Preparing the Batch Template for Use This section goes through each of the modules included in the “nophysio_nob0” batch template for SPM8 and contains information on which options need to be updated to carry out preprocessing on the specific data at hand. Modules with no fields needing updating are excluded from this list. The goal is to set up the batch script for the first subject to be run. Once the edited batch script has been set up for one subject, the multiple-subject-analysis script included in the SPM8_Batch_Templates directory can be used to run it on all the subjects of a study (see below). NOTE: You will want to save the batch script with a different name in order to not overwrite the original template file!!! It is recommended that this new name somehow reflect the study or analysis in which the data will be used. When saving an updated version of the batch, use the File -&gt; Save Batch and Script option; this will create both the batch script file and the associated _job file. 3D to 4D File Conversion A. 3D Volumes – This should be changed to include the volumes of fMRI data to be processed (any volumes acquired before steady-state should NOT be included here). If the fMRI data are in a single 4D file, individual volumes can be displayed in the file selection window by changing the “1” in the white box on the right-hand side of the window to “1:400” and hitting the tab key. B. Output Filename – This string should begin as a number followed by “_TASK_short.nii”. TASK should be replaced with some word by which you can identify the data (e.g. “EmoReg”, “MemRecon”, “DisgVigns”). Change the number in this string to the subject ID of the first subject you wish to run. Slice Timing A. Number of Slices – This should be changed to the number of slices per volume of the functional data. B. TR – This should be changed to the TR of the functional data. C. TA – As mentioned in the batch editor window help box, this value should be set to TR – (TR/#slices). D. Slice Order – This needs to be set to a list of integers representing the temporal order in which the slices were acquired. There are helpful instructions in the bottom box of the batch editor window on how to enter long lists easily. E. Reference Slice – Unless you know you want to set it otherwise, this should be changed to the number of the slice that was acquired half-way through each TR. If the functional slices were acquired in “alternating_increasing” and there were 36 slices, this would be slice 35. VBM8: Estimate and Write A. Volumes – This should be set to the T1 anatomical image acquired in the same imaging session as the functional data. B. Tissue Probability Map – This needs to point to an image template sitting in a sub-directory of the SPM8 Install Directory (see section 0 above). Specifically, select the file: [SPM8 Install Dir]\\toolbox\\Seg\\TPM.nii. C. Dartel Template – Similar to the Tissue Probability Map, this needs to point to an image template that exists in the VBM8 toolbox: [SPM8 Install Dir]\\toolbox\\vbm8\\Template_1_IXI550_MNI152.nii. Image Calculator (the first one) A. Output Directory – Set this to the directory in which you want the preprocessed results written for the first subject of the study. Image Calculator (the second one) A. Output Directory – Set this, again, to the directory in which you want results written. Deformations (the first one) A. Output Directory – Set this to the same directory as in the Image Calculator modules. Deformations (the second one) A. Image to base ID on – When the functional data are spatially warped to standard space, the output will be matched to have the same voxel size and dimensions as the image entered here. If you do not have such an image, there is one called EPI_resamp.nii in the SPM_Batch_Templates directory. This image has a voxel size of 3.75 x 3.75 x 3.75mm3 and dimensions of 49 x 58 x 49. It is a resampled version of the MNI-152 EPI template that comes with SPM8. B. Output Directory – This should be set to match the output directory listed in the previous modules. QArunall A. Orig_anat – This should be set to the same T1 anatomical image file as the “Volumes” item in the VBM8: Estimate and Write module. B. Orig_fmri – If the functional data are in a single 4D file, that file should be selected for this item. If the functional data are in multiple 3D files, all of those files should be selected here. 5.3.8 Running a Single Subject and Checking Results Once the above edits have been made to the batch script, it is time to test it. Since the script is already set up for the first subject in the study, simply hit the green arrow near the top of the Batch Editor window to start the script. It will probably take on the order of 20-30 minutes to complete, depending on your computer hardware. Throughout the processing, various pieces of information and progress notes will get displayed in the main Matlab window. Matlab may also pop up several dynamic figures of plots and progress bars. The processing also takes up a lot of Matlab’s resources, so opening and saving files in the editor may be very sluggish while it is running. If everything went as planned, the last thing displayed in the Matlab window should include: HTML file written: [FILE] Done ‘QArunall’ Running ‘Move/Delete Files’ Done ‘Move/Delete Files’ Done The final output directory will contain roughly 450Mb of files, depending on the sizes of the original image files. The QArunall module creates a series of quality-assurance pictures, plots, and movies, and ties them together into a single HTML file. This file is located in the [OUTPUT_DIR]\\QA_summary directory and is called QA_summary_display.html. Double-clicking the file should open it in your default web browser. More details on what’s included in this summary and information on determining if the subject has usable fMRI data can be found in the documentation file QA_Output_notes.docx. For now, if all the pictures look brain-shaped the batch script successfully ran to completion for this subject. 5.3.9 Running the Batch Script on Multiple Subjects Unfortunately, the SPM graphical interface cannot run the processing batch script on multiple subjects. However, finding the occurrences of the subject ID in the “_job.m” file associated with the batch script, replacing them with the subject ID of the next subject, and rerunning the batch script is relatively straight-forward. This is basically what the run_multi_spm8_template.m script is written to do. As with the batch script processing, there are a few changes that will need to be made to the multi-subject script in order to make it work with the data at hand. Open the run_multi_spm8_template.m file in the Matlab editor (in the main Matlab window, navigate to the…\\SPM_batch_templates\\SPM8 directory and double-click on the file). Save a copy of the file with a new name pertaining to the study/analysis at hand using the Save As option. You may want to save this new copy in the directory containing the edited batch script file. The four things that need to be changed in the multi-subject script file are: - The function name (line 1) – This must be changed from run_multi_spm8_template to whatever the name of your copy of the file is called (minus the “.m”) - Subject_list (line 6) – This should be changed to include the subject IDs of all the subjects to be run. Each ID should be encased in single parentheses and followed by a semi-colon (except for the last subject ID, which should NOT have a semi-colon after it). - Base_dir (line 9) – This should be changed to the path/directory containing all the subject directories (see section above on how to best format the data directory structure). - File_to_edit (line 15) – This should be changed to the path/filename of the _job.m file associated with the edited version of the SPM batch script created in section 4 above. There is one important “link” between the multi-subject script file and the batch script file that must be maintained for the multi-subject script to run properly. The batch script file must be set up to run the subject whose ID appears first in the batch script subject_list. There are a couple ways to handle this: If the batch script was set up and run on the first subject in sections 4 and 5 above and no changes were made to the files in the resulting output, simply include the first subject in the multi-subject script’s subject_list. This will rerun the processing of that first subject, but this will not cause any issues. If you do not want to rerun the processing of the first subject, open the “_job.m” file in the file_to_edit variable above and run a find/replace (Ctrl+f) replacing all instances of the subject ID of the subject who has already been run with the subject ID of the first subject in the batch script subject_list. Once the multi-subject script has been edited, it can be run by simply navigating, in Matlab, to the directory containing the script file and typing the name of the file (excluding the “.m”) into the Matlab command line. The Matlab window should then display the first subject’s ID followed by the beginning of the processing batch output. If all goes well, the script will run the processing for each subject once the previous subject is complete (thus, the entire script will take about 20-30 minutes per subject to complete; this is a good task for your computer to do overnight). If the processing fails for any subject for any reason, it will make note of it and move on to the next subject. When the script has attempted to run the script (successfully or not) for every subject in the list it will display two lists to the Matlab window: “Good Subjects” is a list of the subjects for which the processing did not encounter any programmatic errors (success here makes no comment on the quality of the data); “Bad Subjects” is a list of subjects for which the processing did not complete due to an error in attempting to run one of the processing modules. 5.3.10 Quality Assurance Output Descriptions A Pictures and Metrics From the QArunall SPM Module This documentation contains information on the various quality assurance (QA)-related output created by the “QArunall” module for SPM. The QArunall module was written locally by John Graner and provides quickly-viewable pictures, movies, and plots linked by a single html file. It does not provide a QA “result” for each data set, relying on the user to interpret the information presented. Below is a list of each element created by the module and linked to in the HTML file. Descriptions and examples are provided when relevant as well as things to look out for. Note: the word “picture” is used to refer to the visual presentation of data in the HTML file (these are just .png files); the word “image” is used to refer to the actual MRI data. An example QA HTML file is located in the documentation zip archive on Keoki in …/Graner/Data/SPM_Preprocessing_Things. After unzipping the archive, the summary file is .../QA_Summary/QA_summary_display.html. Original Anatomical Description. Three pictures of the original T1 anatomical image associated with the fMRI data. Each picture is the center slice of one of the three image dimensions (axial, sagittal, coronal). Note that the center image slice will most likely not correspond to the center of the brain. The brain may appear off-center and at a slight angle in each picture. However, the entire head should be contained within each of the three pictures. QA Considerations. Make sure the brain image looks like a brain image. If any structural abnormalities are suggested by the pictures, open the original T1 image in a full image viewer (e.g. SPM’s “Display” module) to verify. Processed Anatomical Description. Three pictures of the T1 anatomical image after it has been through preprocessing. These pictures should contain only brain tissue; the skull and other surrounding tissue should have been removed. The brain should also appear to be centered and not rotated in the three pictures, as it should now be registered to a standard image template (MNI-152). QA Considerations. Look for any areas around the edge of the brain where the skull-stripping failed and there is still non-brain tissue/bone in the image. The transformation into standard space involves a non-linear warp of the anatomical image. When this fails or goes awry portions of the image may get improperly warped and the brain will correspondingly look deformed. Look for any areas where the brain anatomy looks displaced, oddly shaped, or oddly sized. Original fMRI Description. Three pictures of the first volume of the original fMRI data. As with the original anatomical images, each picture should contain the brain (unless images were only acquired over a specific portion of the brain), but the brain might not be centered or aligned. QA Considerations. Look for any spatial artifacts in the pictures, such as large regions of signal drop-out, large “protrusions” from the brain, etc. Note that these pictures are of the first volume of the original fMRI data before ANY preprocessing has been done, so they may show a pre-steady-state volume. In this case a slice-wise banding artifact (each slice has a different average signal value and is therefore distinctly visible on the two pictures not in the slice acquisition dimension) may be present but not indicative of bad data. Preprocessed Functional Description. Three pictures of the first volume of the preprocessed fMRI data. This version of the fMRI should be centered in the pictures and not rotated. QA Considerations. Look for any spatial artifacts in the data (a remaining banding artifact, deformations of the brain, or places with “protrusions” from the brain). Double-check that the brain looks to be straight in each picture and that the whole brain is visible in each picture (and not cut off at an edge). Anatomical Contours over Final fMRI Description. Three pictures of the preprocessed fMRI with signal intensity contours of the preprocessed anatomical image overlayed in red. The red contours are created from the processed anatomical image and will usually (although not specifically) follow contours between CSF, grey matter, and white matter. QA Considerations. The overall purpose of these pictures is to make sure the processed anatomical image and processed fMRI image are spatially aligned. Look to make sure the outline of the brain in the contours matches the outline of the brain in the fMRI image. Note that there may be some extension (maybe one or two voxels) of what appears to be the edge of the brain in the fMRI image beyond the superior edge of the contour. The corpus callosum should also be distinguishable as a contour in the sagittal picture. This contour should encapsulate a region of relatively decreased signal intensity in the underlaying fMRI data as well. The left and right ventricles should be similarly matched in both the contours and the fMRI data. If no B0 inhomogeneity correction was done there will most likely be some “leak” of fMRI signal beyond the anterior portion of the outer contour. Finally, the general outline of the gray matter should be visible in the contours and match with slightly higher signal intensities in the fMRI data. It is important for both the internal structures and the outer boarder of the brain to be aligned in these pictures. FMRI Motion Plots: Rotation Parameters Description. This plot shows the three rotation parameters used by SPM when motion-correcting each volume of the fMRI data. The ordinate has units of degrees (usually small) and the abscissa has units of volume number. QA Considerations. This plot is rather straight-forward. Sharp, sudden increases or decreases in the plot represent TRs where the subject rotated his/her head suddenly. Gradual changes represent slower rotations over time. In many cases it is the sudden, sharp movements that may lead to issues. More information on the presence of these can be obtained from the other motion-related plots and movies, described below. FMRI Motion Plots: Translation Parameters Description. Very similar to the rotation parameter plot, this plot shows the three translation parameters used by SPM when motion-correcting each fMRI volume. The ordinate has units of millimeters. QA Considerations. Interpretation of this plot is very similar to that of the rotation parameter plot. Make note of any sharp changes in parameters between TRs. FMRI Motion Plots: Time Points to Censor Description. This is a single number as well as a plot showing the TRs suggested for censoring when carrying out subsequent analysis of the fMRI data. The plot has values of either 1 or 0 for each image volume. A 0 indicates the presence of significant motion around that volume, suggesting it should be excluded from analyses. The number displayed after the text “Time Points to Censor” is simply the number of volumes labeled with “0” in the plot. A volume is suggested for exclusion if the total motion between it and the previous volume is above a certain threshold. QA Considerations. This plot, the translation parameter plot, and the rotation parameter plot are displayed at the same size and x-scale in the HTML file to allow easy comparison between the timing of the volumes suggested for censoring and the timing of changes in the parameter plots. Make sure the volumes suggested for censoring correspond to sharp changes or peaks in the parameter plots; the three plots should sort of “tell a consistent story.” FMRI Center Slice Movie Description. This is a GIF showing the center sagittal slice of the processed fMRI data at every TR. The white bar at the bottom of the movie represents how far through the scan time-course the current picture is. QA Considerations. There are two main motivations for this movie. The first is to look for residual motion in the image that was not fully corrected in the motion correction step. This is often the case where there are many censored volumes or large jumps in the motion correction parameters; although some of the motion effects are removed the correction is usually not completely successful. Even if residual motion is visible in the movie (the brain appears to jump or jitter), however, it will not negatively impact analyses of the data so long as it occurs during time-points marked for censoring and censoring is in fact carried out during analysis. The second motivation behind this movie is to look for any temporal artifacts in the processed fMRI data. These can be slightly more insidious and less predictable. Possible things to watch out for include the sudden appearance of bands across the image in certain volumes, an intensity pattern that moves through the entire image as time progresses (e.g. “waves” of signal intensity moving through the brain), or the appearance of static noise inside or outside the brain in certain volumes. In general, look for any systematic or sudden change of signal that appears to be unrelated to the brain itself. FMRI Variance Movie Description. This is a gif showing the variance of each voxel time-course. The movie cycles through sagittal slices of the brain in the left-right direction (this could be left-to-right or right-to-left, depending on the data). QA Considerations. The goal of this movie is to look for any systematic spatial patterns in the voxel variances across the image: straight lines of increased variance, a grid pattern throughout the movie, etc. The variances of voxels on the edge of the brain should be greater than those of voxels inside the brain (the variances of voxels on the edge of the brain will be larger due to a larger degree of signal change brought about by any motion which causes a voxel to shift from being in the brain to being outside the brain). This should manifest as a visible outline of the brain in the movie. 5.4 QA - FMRIPrep Created on 2022 Sep 14 by John Graner. Ported to bookdown on 2022 Sep 15 by Nathan Muncy. 5.4.1 General Information QA Result Options Each analyzable portion of data gets its own result from the following options: Green – Data look good and are usable Yellow – Data may be usable and can continue to analysis, but results should be checked for a specific artifact or concerning feature Red – Data are not usable QA Evaluation Guide The QA information provided by fMRIPrep serves two purposes. The first is to determine if the preprocessing steps worked successfully and as expected. The second is to determine if the input data contain signal variance, due to noise sources, that will interfere with the accuracy of subsequent analyses. Each section of the QA output provides a piece of information relevant to one or both of these determinations. Note that evaluation of the quality of the input data is somewhat dependent on the planned analyses. For example, an artifact in the orbital frontal cortex might not matter if the analysis plan calls for investigating only the parietal lobe. Thus, while this guide can provide help for interpreting QA output, it cannot offer advice on how to evaluate the importance of what the output reveals. Summary This is an overview of what was processed and the output spaces to which it was normalized. Make sure the files listed match what is expected. If anything is missing or if there are extra files listed, follow up by checking the BIDS directory, original data directory, and any scan-day notes. 5.4.2 Checking Anatomical Images Anatomical Conformation This is an overview of the anatomical images and their output grids. Check to make sure these are as expected. Brain mask and brain tissue segmentation of the T1w This figure shows the grey matter/white matter/cerebral-spinal-fluid segmentation of the T1-weighted anatomical image. Ideally, the various color contours should follow the tissue changes through the brain. Dots are ok. The red line should trace the edge of the brain. It won’t be perfect (e.g., the brain often edges out just past the line), but look for missing chunks. Inferior insula and temporal lobe tend to be problem areas The magenta lines trace ventricles and sulci. Like the red line, the magenta lines may be slightly imperfect, but they should generally follow the shape of all the ventricles and sulci. The blue line should tightly trace the white matter. The mid-sagittal slice often appears strange, but you should see in other slices that the “fingers” of the white matter are well-traced. The blue lines are generally more precise than the red and magenta. If there are significant errors in the contour lines, visually check the T1w image in the BIDS directory to look for any artifacts or other quality issues. If there aren’t any visible problems with original data, rerunning fMRIPrep might solve the issue. Spatial normalization of the anatomical T1w reference When the mouse cursor is hovered over this figure, it switches between the standard space anatomical template and the result of attempting to spatially register the input T1 anatomical image to it. Ideally, they should align very closely. Check for global alignment of the two images: Is the subject’s brain the same size as the template brain, and are the brains laid directly on top of one another (no relative rotation or translation)? Then, look for alignment of local features: Check to see if any region of the subject’s brain is inflated or deflated relative to the template Check that specific structures are roughly aligned: Select an anatomical landmark (a sulcus or gyrus is a good choice), and see whether the landmark is in the same place, and is of roughly the same size, in the subject and template brains. The landmarks will often look a little different in the subject and template brains; subject brains will usually have slightly different cortical folding than the template, and the template will always be blurrier and brighter than the subject brain. However, you should see that the same general landmark is in roughly the same place in the two brains. Similar to the previous section, if there are significant errors in the alignment, visually check the T1w image in the BIDS directory. If there are no visible concerns with the original image, try rerunning fMRIPrep. B\\(_0\\) field mapping This section displays information on inhomogeneities in the scanner magnetic field due to the presence of the participant’s head. When the mouse cursor is hovered over the figures, they will switch on and off a display highlighting the degree of inhomogeneity in each voxel. Inhomogeneity is most common in the following areas: in and below the orbitofrontal cortex, medially below the center of the brain, bilaterally outside the brain below the ears, some regions outside the skull anterior and posterior. Most of the space within the brain itself should be free of highlighting. Portions of the orbitofrontal cortex (the mid-line front bottom of the brain) will probably have some highlighting. Check the figures to make sure there isn’t much inhomogeneity reported inside the brain itself. The regions of inhomogeneity should appear as cloudy blobs in the figures, without any slice-wise patterns. If the inhomogeneity looks odd, visually check the image used to do the field mapping. This may be a reverse-phase-encode EPI acquisition or a direct field map. 5.4.3 Checking Functional Images Summary Make sure the values of the following parameters are as expected: Repetition time (TR), Phase-encoding (PE) direction, sequence description, Slice timing correction, Susceptibility distortion correction, Registration. Non-steady-state volumes: The number of automatically detected non-steady-state volumes should be 2-4. If this number is larger, visually inspect the first 10 or so volumes of the functional data in the BIDS directory and the ICA output (see below) to check for artifacts. Confounds Collected: There will be a long list here (initially collapsed out of view; click the arrow next to the label to open it). Check the highest number of the “motion_outlierYY” confounds. If this number is greater than the total number of volumes in the image times 0.2 (or whatever motion-censor limit the current study is using), the run should be marked as poor and not used in analysis. Susceptibility distortion correction The goal is to determine whether the quality of the image has improved after distortion correction. Hovering the mouse cursor over the image slices will cause the figure to shift between the “before” image and the “after” image. The “before” image is the EPI data before distortion correction, and the “after” image is the post-distortion-correction version. The blue contours in the figure are white-matter boundaries derived from the T1-weighted anatomical image. Ideally, the white matter boundaries of the EPI should more closely match the blue contours after distortion correction. The two main signs of improvement are: The white matter boundaries in the EPI better align with the blue contours Obvious distortions/deformations are attenuated (e.g. the cerebellum goes from being obviously misshapen to looking about right; the posterior and anterior portions of the brain get “pushed in” or “stretched out” to better match the blue contours). If the image has generally improved, even by a little, it’s fine; if it has not improved, or has gotten worse, make note of it (fMRIPrep can be rerun excluding the distortion correction step). One possible strategy for this section: Zoom in on the image pane, and try to find slices where one image appears better than the other. Then, zoom out and see whether the improved image is the “before” or “after”. After doing this for several slices, you’ll get a feel for whether things have generally improved. As you get practice with this, you’ll figure out what slices commonly show signs of correction. Here are some signs that often stand out: Distortions around the frontal pole are common. Corrections for these often results in better white matter alignment in the central axial slice, the axial slice to its immediate right, and sagittal slices that display large swaths of white matter. Additionally, you may notice that the anterior portion of the central axial slice looks strangely “pushed out” or in one image, but more naturally rounded in the other; that’s also a sign of improvement. In the midsagittal slice, or nearby slices, the orbital frontal region may be “stretched out” before correction but is brought in after correction. The same may be true for the superior and inferior of the posterior portion of the brain. Alignment of functional and anatomical MRI data Red lines represent tissue contours of the structural image and should match the “fixed” image almost perfectly. The “moving” image (the functional) is the one to evaluate. Ideally, tissue type boundaries in the moving image also line up with the blue contours. Portions along the outside of the brain may not align with the blue contours as well as central portions. Brain mask and (temporal/anatomical) CompCor ROIs This figure shows voxels algorithmically identified as containing noisy time-courses, either because they are located in the white matter or CSF or because their time-courses have characteristics that make them look mostly unrelated to neural activation. Check that the red line roughly traces the edge of the brain (will not be exact) The voxels inside the magenta lines should be in the white matter and CSF. They will not trace the full boundaries of white matter or ventricles, but the areas they trace shouldn’t include much grey matter. It can be hard to tell what’s grey matter in these images; just check that the boundaries look plausible. The blue regions correspond to the noisiest voxels in the brain; they’re mostly likely to be near CSF and the brain stem. These are only notable if they’re drawn somewhere you rarely see them (e.g. there shouldn’t be a big chunk in prefrontal cortex). Variance explained by t/aCompCor components Note: If the study analysis plan does not include any CompCor regressors, ignore this section completely. These plots show the number of CompCor components necessary to explain given amounts of variance in the ROI time-courses. The curves on the plots should generally start vertical-ish and become more horizontal as you travel along the x-axis. If the curves do not look as expected, double-check the CompCor ROIs from the previous section. Rerunning fMRIPrep may help. If it does not, make note of it, as these regressors may need to be excluded from further analysis. BOLD summary Compare the DVARS and FD graphs. FD is a measure of head motion, and DVARS is a measure of signal intensity change across the brain. Signal intensity change will spike when head motion spikes. Check for any spikes in signal intensity change that are not correlated with head motion. If there’s a spike in DVARS without any spike in FD, that’s notable - it could indicate the presence of an unknown artifact. The “carpet plot” below the FD curve shows columns of time and rows of voxels. The greyscale in the figure maps to voxel signal intensity. Inspect the carpet plot for any patterns that your eye/brain notices. Examples include vertical stripes, horizontal stripes, and grid or checkerboard patterns. Vertical lines indicate a change in intensity at the same time point in many voxels. If there is a spike in the FD plot that corresponds in time with a vertical line, the line is most likely due to head motion. Horizontal lines standing out in the carpet plot may indicate several voxels with time-courses that are unlike other voxels around them. Figure 5.1: Example of head motion showing up as vertical patterns in the carpet plot. Correlations among nuisance regressors Strong correlation between 5 or more regressors may indicate some sort of artifact in the data. Check the ICA components (see below) to try to get a sense of where the correlation may be coming from. ICA Components classified by AROMA This is probably the least concrete portion of the QA. Each component has a spatial map, a time-course, and a representation of the time-course in the frequency domain. Figure 5.2: Example ICA-AROMA component, including spatial map (left), time-series (right, top) and frequency power spectrum (right, bottom). Components labeled with green letters and with green time-courses are judged by the AROMA algorithm to be non-noise. That is, they are judged to be possibly associated with neural activity. Conversely, components labeled with red letters and with red time-courses are judged to be noise. These components can reveal artifacts present in the data that are not easily detectable through volume-by-volume visual inspection. The primary QA goal for this section is to see if there are any signal components mislabeled as red that might negatively influence the results of subsequent analyses. However, the components in this section can also provide additional insight into patterns in the data that might be creating notable results in other sections of the QA output. Spatial Characteristics of Components: Note: The characteristics and examples below are not exhaustive. Artifact components may have the following general spatial characteristics: Well-defined regions with sharp cut-offs Span entire slabs of the image (in any of the 3 dimensions) and either seem to be present only in those slabs or repeat in different slabs, leading to a striped appearance Follow closely along the edge of the brain Speckled throughout the brain Strong patterns that don’t appear to follow the brain anatomy (e.g. circles, large “slashes”, “ripples”) Figure 5.3: Example spatial maps of artifact/noise components. Neural components may have the following general spatial characteristics: “Blobs” with softer edges Seem to follow or match with the gray matter structural or functional anatomy of the brain Bilateral (mirrored left-right across the center of the brain) Look like known functional networks Figure 5.4: Example spatial maps of activation-related components. Temporal Characteristics of Components: Artifact components may have the following general temporal characteristics: High-frequency, “squiggle-y” look throughout Noticeable upward or downward slope across the whole time-series Peaks at specific frequencies and little power in other frequencies Figure 5.5: Example time-courses of artifact/noise components. Neural components may have the following general temporal characteristics: Periods of increased signal and decreased signal that last for several time-points each Smoother rises and falls in the intensity time-course Figure 5.6: Example time-courses of neural activation-related components. "],["programming.html", "Chapter 6 Programming 6.1 Introduction 6.2 Best Practices 6.3 Command Line Interface 6.4 Documentation", " Chapter 6 Programming Written on 2022 Sep 04 by Nathan Muncy This chapter is under construction. 6.1 Introduction The goal of this chapter is to provide general guidance on a number of topics with an aim to standardize how code is written in the LaBar Lab. Standardization will (1) speed-up the development and trouble-shooting process and (2) support maintenance and use efforts when students and researchers have left the lab. 6.2 Best Practices A programming adage says that “Code is written once but read many times”. Best practices guide decisions that programmers make when writing their code, ranging from how to name variables to the structure of entire projects, so that their (brilliant) work is accessible to, and maintainable by, others. Many articles, blogs, and books have been written about best practices, so the aim here is not to exhaustively describe all recommendations but to provide some basic principles. In addition to more language-specific recommendations below, a few guidelines are applicable in any language: Avoid deep nests of logic. Keeping nested logic as simple as is reasonable will help the next person follow your code (including your future self). Such nests show up in for-loops, conditionals, and method calls. If you are finding yourself spelunking in nests (I try to keep it less than 3 levels) save your future self the headache and restructure your code. Relatedly, have each line of code do only one or two things. A complicated “one-liner” is only cool for 15 seconds, and after that will convolute the code for everyone (including you). Have meaningful names. Names for variables, functions, methods, modules etc. should be simple and descriptive. We do not want to remember what i means in this current loop, nor having to re-read documentation about what function_a and pls_work do. A good name will be unique in the project’s namespace, human readable, and informative. Minimize technical debt. Time spent up-front is time saved on the life of the project. Take the time to structure your code well and write documentation. 6.2.1 Python Python has an established history of best practices since PEP8, and modern IDEs like VS Code are configurable to help conform to these (and more modern) recommendations. I recommend using a linter (and it is very easy to install and use in VS Code): linters serve to check for syntax and/or style issues by referencing a predetermined guide. The guide Flake8 is a preferred style that detects both syntax and style issues. Additionally, auto-formatters such as Black can save you the effort of formatting your code according to Flake8 or other recommendations (and can also be installed in VS Code). As PEP8 is well established and conforming somewhat automated, here I reiterate and make a few suggestions: Variable, function, method, module, and package names should be named in snake_case Class names should use PascalCase Every python file (except maybe __init__.py) should have a __doc__ attribute, preferably at the top of the file Functions, classes, and methods should also contain a docstring Docstrings should be formatted to the Numpy style Format comments in sentence case, and use comments to explain the functionality (not syntax) Decide whether procedural, functional, or object-oriented programming best suite your task (Section ??) Avoid editing code for each use – supply a command-line interface (Section 6.3) if argument parameters are needed Encapsulate work within functions, and even simple scripts should put work within a main() function. 6.3 Command Line Interface A resource or package is often developed that is a bit flexible, allowing for the functionality to be adjusted by user input. For instance, a general set of steps for functional MRI preprocessing can be written in a script without specifying certain values like the participant and session IDs, task names, or the input and output directories. Such a script would be very useful and portable. It is not recommended, however, to structure your code such that the user must open and edit the code for each use (e.g. new subject, session, or directory path). Rather, build a method of capturing user inputs and supply documentation on what inputs are allowed and their functionality. This section details how to setup such a command line interface. 6.3.1 Python Python makes requiring, capturing, and specifying command line inputs very easy with the standard argparse package. Here we will use this package to set up a small script. The __doc__ and import In a python script called example.py, I first set the __doc__ attribute detailing what work the script does and how to use the script from the command line, using the Numpy style of docstring formats (Figure 6.1, lines 1-10). In practice I actually tend to write the __doc__ attribute last, but this is just an example. Figure 6.1: __doc__ attribute and imports. An Examples section is an important aspect of the docstring, and here I supply two examples illustrating the different use cases (lines 7-8). Following the the __doc__ attribute, a few packages are imported: sys will help with counting the number of arguments, textwrap will help with formatting option defaults, and from the argparse package the two most critical classes are ArgumentParser and RawTextHelpFormatter that are used in capturing and communicating the arguments (lines 11-13). Specifying arguments Next, we will define a function called get_arguments to receive arguments (Figure 6.2). The first step is to initialize an object called parser that will be used to specify arguments (lines 18-20). Here we specify the description and formatter_class attributes, by setting the description attribute to __doc__, we will be able to include our __doc__ attribute (and the supplied examples) in the help section. There are many argument configurations possible when using ArgumentParser, and as the documentation is recommended, here we will detail capturing a few strings. Lists, boolean, and other object types are also possible to capture. First in our example we build an optional argument (Figure 6.2, lines 22-33); optional arguments are useful if the developer can anticipate a common argument from the user but still wants to allow the user to specify something else. In our instance, perhaps most people will think “mango” is their favorite fruit (set as the default on line 26), and in the real world perhaps you can anticipate data consistently stored in a specific location. In unpacking lines 22-33, we can see that we are adding an argument available to the user which is accessed via --fruit (Figure 6.2, line 24), the parameter supplied should be a string (Figure 6.2, line 25), the default value is “mango” (Figure 6.2, line 26), and the help provides instruction as well as specifying what is the default value (Figure 6.2, lines 27-32). In this way, if the user does not elect to use the --fruit option, then the code will use the default value of “mango”. Figure 6.2: Function to require and capture arguments. Next, we specify a required argument. While the code will not require an optional argument to run, and will use the default value if one is specified, python will stop if it detects that (a) a required argument exists, and (b) the user did not specify it. To make an argument required, one merely needs to specify a required option (Figure 6.2, line 41), but a few extra steps will help organize the help of your code. On line 36 I start a new argument group called “Required Arguments”, the help of these arguments will be printed separately from the optional arguments, and with this ordering (optional first, and then required arguments), the required arguments will appear closest to the command prompt. Next, I build a required argument (Figure 6.2, lines 37-48) in this new “Required Arguments” group, specifying the option required=True. You’ll notice that I used both short and long options (-n and --name, respectively; Figure 6.2, lines 38-39), python developers tend to use long options as they are more informative, and I like to include a short option for required arguments to (a) visually distinguish between optional and required arguments in examples and (b) save on user typing time as these options will always be used. Finally a help and example is supplied (Figure 6.2, lines 42-47). The various options and their parameters are returned by the get_arguments function on line 55. Finally, a common practice is to for the user to execute a script or package without any arguments specified in order to print a help. To accomplish this, we trigger the print_help function of the parser object if no arguments are used and then terminate the script (Figure 6.2, lines 51-53). The work Now that optional and required arguments have been specified, we are ready to put those arguments to use! Figure 6.3 supplies the main function where the arguments are assigned and then used. On line 61 I first access the object returned by the get_arguments function and parse the arguments, parsing assigns each argument to its own attribute that is accessible via the supplied option. I can then set the values supplied by the user to new variables (Figure 6.3, lines 62-63). The user_name and user_fruit variables, which contain default and user-specified parameters, are now able to be used in the script. Line 66 simply uses the variables to print a sentence out to the console. Figure 6.3: Use arguments to print. By encapsulating the work in main and including the dunder __name__ conditional (lines 69-70, also see this) we can set our script up to be executable from the command line. Let’s start testing with this, and here is the code for your convenience: &quot;&quot;&quot;Print user&#39;s name and fruit. Capture input from the user and print out responses. Examples -------- python example.py -n Nate python example.py -n Nate --fruit orange &quot;&quot;&quot; import sys import textwrap from argparse import ArgumentParser, RawTextHelpFormatter def get_arguments(): &quot;&quot;&quot;Capture and parse arguments.&quot;&quot;&quot; # Initialize argument parser. parser = ArgumentParser( description=__doc__, formatter_class=RawTextHelpFormatter ) # Capture optional arguments. parser.add_argument( &quot;--fruit&quot;, type=str, default=&quot;mango&quot;, help=textwrap.dedent( &quot;&quot;&quot;\\ Specify a fruit. (default : %(default)s) &quot;&quot;&quot; ), ) # Capture required arguments. required_args = parser.add_argument_group(&quot;Required Arguments&quot;) required_args.add_argument( &quot;-n&quot;, &quot;--name&quot;, type=str, required=True, help=textwrap.dedent( &quot;&quot;&quot;\\ Specify user&#39;s first name. e.g. &quot;-n Nate&quot; or &quot;--name Nathan&quot; &quot;&quot;&quot; ), ) # Trigger help and exit if no options are used. if len(sys.argv) &lt;= 1: parser.print_help(sys.stderr) sys.exit(0) return parser def main(): &quot;&quot;&quot;Print user&#39;s favorite fruit.&quot;&quot;&quot; # Get user-specified arguments. user_arguments = get_arguments().parse_args() user_name = user_arguments.name user_fruit = user_arguments.fruit # Conduct work. print(f&quot;\\n\\tThe favorite fruit of {user_name} is {user_fruit}.\\n&quot;) if __name__ == &quot;__main__&quot;: main() Running the script If we recall lines 51-53 (Figure 6.2), our script should print out a help if we execute it without any arguments. When we do so, observe how the help is built from both our __doc__ attribute (Figure 6.1, lines 1-10) as well as the help options specified in the get_arguments function. Additionally, as we created a separate “Required Arguments” group, these options are separated from the optional arguments (Figure 6.2, line 36). Figure 6.4: Executing script without arguments to print help. Next, the --name should be required, so what happens if we try to run the code with only the optional argument specified? Notice that a condensed help is printed to the console, detailing the usage and giving the error of missing required arguments. Figure 6.5: Executing script without required arguments. Finally, we can test both the optional and required arguments. Figure 6.6: Executing script with optional and required arguments. 6.4 Documentation Generally speaking, documentation is used for structuring and communicating your code or program. As documentation is used at different levels of programming and for different purposes so do its methodologies. At the planning stage, documentation can be used to sketch out the scaffolding of your program, planning what functionality will be written in which file and also providing a general organization to the project. At the development and maintenance stage, documentation both communicates the functionality of blocks of code and provides a framework to guide code insertions and refactoring. At the implementation or execution level, documentation provides information the user needs to effectively use the code and resolve any errors. Documentation also serves as a “knowledge capture”, an important aspect when personnel turnover every few years. By documenting code well, a future user will know (a) what it does, (b) how to use it, (c) how to resolve errors, and (d) how to maintain it. As documentation serves many functions, it is found in many locations. Documentation about the structure and functionality of the code is often found within the code itself in the form of comments, and documentation about how to use the code is typically found within a help function or file. Finally, documentation about the project as a whole can take the form of a README, book, or webpage. As documentation can serve many functions and be located in different forms and media, here we provide some guidelines with an aim of standardizing practices. Standard practices for documentation ensures that both users and developers can find, read, and understand the type of documentation needed for their task. 6.4.1 Python A range of practices and tools are available to help with documentation in python, from guidelines to software that can build html pages from properly-formatted documentation. When leveraged these practices and tools can make code quite accessible. Comments The role of comments is to describe what a certain block of code does. Write the comment for an audience that can read code, that is, explain what is happening and why while avoiding an explanation of the syntax. Comments should be used like salt – too little or too much is problematic while the right amount makes your mother proud. Pairing with proper naming conventions should allow the next reader to fully understand what is happening in the current section of code. Comments should be written and sentence case and preceded by a single “#” and space. Line length guidelines apply to comments, so longer comments should take the form of a left-justified block. Figure 6.7 provides a few examples of less-than-helpful comments. Figure 6.7: Examples of bad comments. While any comments (as long as they are not wrong) are better than the absence of comments, a few grievances can be expressed about this commenting style. First, each comment only describes the syntax, making the comment redundant for anyone that can read the code (and assume your reader can read the code when writing comments!) Second, too many comments are supplied – lines 77, 79, and 81 do very similar things, so their corresponding comments say largely the same thing. All this clutters the code. A better example is shown in Figure 6.8, where a comment is supplied for each section of work. Figure 6.8: Examples of good comments. While the examples here are rather simple, and so the comments still somewhat describe the code, as the functionality becomes more complex it will be important to remember to describe the point of a section of code, not the syntax itself. And if the syntax is very dense and confusing, a comment is definitely warranted to guide the programmer but restructuring is also likely warranted. "],["databases.html", "Chapter 7 MySQL Databases 7.1 Create an Account 7.2 Login 7.3 Create a Database 7.4 Create a Table 7.5 Insert Data 7.6 Select Data 7.7 Python and MySQL 7.8 R and MySQL", " Chapter 7 MySQL Databases Written by Nathan Muncy. A MySQL server is installed on labarserv2 which allows for project data to be stored within a relational database management system (RDBMS). Utilizing a RDBMS allows the researcher to centralize, better manage, flexibly access, and share their data. Additionally, the dedicated structures will help maintain data consistency across users and projects. This chapter will serve as a brief tutorial and reference for setting up and then using project-specific databases. 7.1 Create an Account Admins of labarserv2 create accounts and grant access to the mysql server and databases. Start by logging into the server as admin $sudo mysql. Current user and host information are available at mysql.user: use mysql; select user, host from user; To create a user (‘test’) identified by a password (‘foobar’), specify the new user name, host or IP, and password: create user &#39;test&#39;@&#39;localhost&#39; identified by &#39;foobar&#39;; create user &#39;test&#39;@&#39;127.0.0.1&#39; identified by &#39;foobar&#39;; Next, permissions to a specific database (‘db_test’), and also tables if desired, can be granted (and note that comments are specified in SQL via --): grant insert, update, select, references, create temporary tables on db_test.* to &#39;test&#39;@&#39;localhost&#39;; -- give select permissions grant all on db_test.* to &#39;test&#39;@&#39;localhost&#39;; -- give all permissions Finally, check that appropriate permissions have been added to the user account and then apply the changes to the server: show grants for &#39;test&#39;@&#39;localhost&#39;; flush privileges; The user should then be able to login to the server via $mysql -u test -p, enter their password (foobar), and access the database (use db_test; show tables;). 7.2 Login Users that have an account can login to the mysql server via $mysql -u &lt;user_name&gt; -p replacing &lt;user_name&gt; with their account name and then supply their password when prompted. Databases to which the user has access can be listed via show databases;. After selecting a database for use (use db_some_name;), available tables can be listed via show tables;. A database can also be specified at login: $mysql -u &lt;user_name&gt; -p db_some_name Exit the server via exit; or quit;. 7.3 Create a Database Admin (i.e. all) privileges are needed by the user to create a database. Accordingly, it is most likely best for labarserv2 admins to create databases and then grant specific (or all) permissions to the user for the specific database. Database creation is accomplished simply via: create database db_some_name; Conventions - The following database conventions are implemented in the LaBar Lab: Database names start with db, e.g. db_project1 and db_project2. This allows for project databases to be identified separately while also being distinct from admin and schema databases Separate databases are made for separate projects, that is, one project should not span multiple databases Database names are succinct, being both descriptive and as short as is reasonable Databases are named in snake case (db_emotion_regulation and not db-emotion-regulation) 7.4 Create a Table Tables are 2-dimensional matrices containing rows and columns of data. Each row must contain unique information and each column contains only one data type. Whether to use long- or wide-formatted tables depends on the user and nature of the data, but long-formatted tables (or at least tidy) is preferable and generally more flexible. The create table syntax is used within a database to both create the table and specify the number and types of columns. Three types of tables (reference, data, and definition) are used in the LaBar Lab, which are prepended with ref_, tbl_, and def_ identifiers, respectively. 7.4.1 Reference Tables When a certain value, such as a participant ID or emotion name, shows up across multiple data tables, the value can be stored in a reference table rather than be used repeatedly across multiple tables. These reference tables, and the fact that values from multiple databases can be queried at once, are the point of RDBMSs. As an added perk, the size of the database is also decreased as redundant values are minimized. Say a project intends to gather information across multiple subjects, each of whom will participate in three sessions, and some measures will occur during all sessions. We can then set up reference tables for the subject IDs, session IDs, and emotion IDs as these values would be common across all measures. To start a reference table for subject IDs: use db_test; create table ref_subj ( subj_id int not null, subj_name char(4) not null, primary key(subj_id) ); Here we created the table ref_subj in the database db_test. This table contains two columns: the first column is titled subj_id, is integer type, and does not allow null values. The second column is titled subj_name, the value uses exactly 4 characters, and also null values are also not allowed. Finally, the primary key is set to reference the ‘subj_id’ column. This key is used to uniquely identify each row in the table, and will serve as the reference value across database tables, and note that not null is required for primary keys. A description of the table is available via the describe table_name command: mysql&gt; describe ref_subj; +-----------+---------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-----------+---------+------+-----+---------+-------+ | subj_id | int | NO | PRI | NULL | | | subj_name | char(4) | NO | | NULL | | +-----------+---------+------+-----+---------+-------+ 2 rows in set (0.01 sec) Conventions - The following reference table conventions are implemented in the LaBar Lab: The table name starts with ‘ref_’ The name for columns involved in the primary key ends in ’_id’ The value which corresponds to the primary key ends in ’_name’ The table name is reflected in column names when possible (e.g. ‘subj’) Following these conventions will increase the ease of joining tables and allow other researchers to more readily understand the data structure. Data can then be added to the table via insert by specifying the desired columns and input values: insert into ref_subj (subj_id, subj_name) values (1, &quot;SUB1&quot;), (2, &quot;SUB2&quot;), (3, &quot;SUB3&quot;), (4, &quot;SUB4&quot;); Check that the reference table is properly specified via select: mysql&gt; select * from ref_subj; +---------+-----------+ | subj_id | subj_name | +---------+-----------+ | 1 | SUB1 | | 2 | SUB2 | | 3 | SUB3 | | 4 | SUB4 | +---------+-----------+ 4 rows in set (0.00 sec) The other reference tables, for session and emotion, can likewise be built (note the variable emotion name length): -- Session reference create table ref_sess ( sess_id int not null, sess_name char(4) not null, primary key(sess_id) ); insert into ref_sess (sess_id, sess_name) values (1, &quot;day1&quot;), (2, &quot;day2&quot;), (3, &quot;day3&quot;); -- Emotion reference create table ref_emo ( emo_id int not null, emo_name varchar(10) not null, primary key(emo_id) ); insert into ref_emo (emo_id, emo_name) values (1, &quot;amusement&quot;), (2, &quot;anger&quot;), (3, &quot;anxiety&quot;), (4, &quot;awe&quot;), (5, &quot;calmness&quot;), (6, &quot;craving&quot;), (7, &quot;disgust&quot;), (8, &quot;excitement&quot;), (9, &quot;fear&quot;), (10, &quot;horror&quot;), (11, &quot;joy&quot;), (12, &quot;neutral&quot;), (13, &quot;romance&quot;), (14, &quot;sadness&quot;), (15, &quot;surprise&quot;); Once completed, we have three reference tables that can be used to interpret repeated values in data tables via join (see below). Such a modular structure, using one table per data type and/or source while accounting for repetitive data, allows for better maintenance and greater efficiency of the database. 7.4.2 Data Tables Continuing with the example started above, data from each measure collected is stored in data tables, with repeated values linked to reference tables. Building data tables uses the same methodology described above, with the addition of specifying foreign keys and thus beginning to make our database relational. Let us suppose that for each session our participants supply PANAS and ERQ ratings as well as a hypothetical in-house measure (EmoFreq) during which participants are prompted to respond how frequently, intensely, and saliently they felt a given emotion (ref_emo) over the previous week. Using the create table syntax, again, we first build a data table for the PANAS questionnaire: create table tbl_panas ( subj_id int not null, sess_id int not null, item_panas int not null, resp_panas int, primary key(subj_id, sess_id, item_panas), foreign key(subj_id) references ref_subj(subj_id) on delete cascade, foreign key(sess_id) references ref_sess(sess_id) ); Here we created a long-formatted table containing four columns – one each for the subject identifier (subj_id), session identifier (sess_id), PANAS item/prompt (item_panas), and participant response to PANAS item/prompt (resp_panas). Participant responses are recorded as integer type, and only the PANAS item number (integer) is used – if it were desired to keep the question language/prompt, rather than question number, then a reference table could be set up (e.g. ref_panas, see conventions above) and the column here would then be named panas_id. Also note that item_panas is specified as not null since participants will see every PANAS item (and this column is used in the primary key). Finally, given that participants may not respond to each item, resp_panas allows for null values. The primary key is specified here as the composition of subject, session, and PANAS item (sometimes termed a ‘composite key’ or ‘natural key’) and serves as as the unique identifier for each participant response. It is recommended to always explicitly specify your primary key. Additionally, explicitly controlling keys helps in setting up proper relationship mappings. If a natural key ought not to be used for a primary key, or data naturally increment, then a column could be set to auto increment and serve as the primary key. Foreign keys are used to reflect that a column is linked to another table, and can also be used to constrain column values (not demonstrated). Here, the columns subj_id and sess_id will not be populated with the actual subject and session names/values, as that would involve a lot of repetitive information, but instead their respective reference table ID. Specifically, we set the subj_id column (tbl_panas.subj_id) to reference the subj_id column of ref_subj (ref_subj.subj_id). Similarly tbl_panas.sess_id references ref_sess.sess_id. Finally, on delete cascade allows records from this current table to be removed if the referenced value in ref_subj is deleted (perhaps due to participant withdrawal). Conventions - The following data tables conventions are implemented in the LaBar Lab: The table name starts with ‘tbl_’ The measure name is reflected in the table name and relevant columns when possible (e.g. ‘panas’) The name for columns that become a foreign key end in ’_id’ when possible Column names for measure identifiers start with ‘item_’ Participant responses start with ‘resp_’ The data tables for the ERQ and in-house EmoFreq surveys can also be made in similar fashion: -- ERQ table create table tbl_erq ( subj_id int not null, sess_id int not null, item_erq int not null, resp_erq int, primary key(subj_id, sess_id, item_erq), foreign key(subj_id) references ref_subj(subj_id) on delete cascade, foreign key(sess_id) references ref_sess(sess_id) ); -- EmoFreq table create table tbl_emofreq ( subj_id int not null, sess_id int not null, emo_id int not null, resp_emofreq_frq int, resp_emofreq_int int, resp_emofreq_sal int, primary key(subj_id, sess_id, emo_id), foreign key(subj_id) references ref_subj(subj_id) on delete cascade, foreign key(sess_id) references ref_sess(sess_id), foreign key(emo_id) references ref_emo(emo_id) ); For the in-house EmoFreq measure, where participants are prompted with an emotion and they respond how frequently, intensely, and saliently they experienced the emotion over the previous week, the item_ column was omitted and replaced with the foreign key emo_id as the prompt is simply the emotion name (and to incorporate our ref_emo table). Additionally, one column per response prompt (frequency, intensity, and salience) is specified. The definition tables section (below), will illustrate a method of making tables more intelligible for the person using them by supplying column definitions and other information needed to interpret the data. Finally, following foreign keys to their source is available in the information_schema database: SELECT `TABLE_SCHEMA`, `TABLE_NAME`, `COLUMN_NAME`, `REFERENCED_TABLE_SCHEMA`, `REFERENCED_TABLE_NAME`, `REFERENCED_COLUMN_NAME` FROM `INFORMATION_SCHEMA`.`KEY_COLUMN_USAGE` WHERE `TABLE_SCHEMA` = SCHEMA() AND `REFERENCED_TABLE_NAME` IS NOT NULL; which yields: +--------------+-------------+-------------+-------------------------+-----------------------+------------------------+ | TABLE_SCHEMA | TABLE_NAME | COLUMN_NAME | REFERENCED_TABLE_SCHEMA | REFERENCED_TABLE_NAME | REFERENCED_COLUMN_NAME | +--------------+-------------+-------------+-------------------------+-----------------------+------------------------+ | db_test | tbl_emofreq | subj_id | db_test | ref_subj | subj_id | | db_test | tbl_emofreq | sess_id | db_test | ref_sess | sess_id | | db_test | tbl_emofreq | emo_id | db_test | ref_emo | emo_id | | db_test | tbl_erq | subj_id | db_test | ref_subj | subj_id | | db_test | tbl_erq | sess_id | db_test | ref_sess | sess_id | | db_test | tbl_panas | subj_id | db_test | ref_subj | subj_id | | db_test | tbl_panas | sess_id | db_test | ref_sess | sess_id | +--------------+-------------+-------------+-------------------------+-----------------------+------------------------+ 7 rows in set (0.00 sec) This is useful when tracking the relationship mappings (such as when working with a new database or when table/column names are less-than-helpful), and requires the references grant for the user. 7.4.3 Definition Tables Few things are more frustrating when working within RDBMSs than not understanding column names or not knowing what data they contain. This is compounded by less-than-intuitive column names that are sometimes necessary due to maximum lengths, as is commonly the case when working on larger projects. Definition tables help assuage these frustrations by supplying definitions or explanations of column names and purpose. As the data table tbl_emofreq (above) has some column names that may be unintuitive, we will specify a definition table to aid in interpreting the table: create table def_emofreq ( col_name enum( &#39;resp_emofreq_frq&#39;, &#39;resp_emofreq_int&#39;, &#39;resp_emofreq_sal&#39; ), col_desc text ); This definition table contains two columns, col_name and col_desc. Column col_name is specified to only accept certain values via enum, which values correspond to the confusing column names in tbl_emofreq. Column col_desc is set to use the text type, which will allow for more verbose strings. A primary key has not been set as this table is not meant to be integrated with other tables but instead to be read by the researcher. Conventions - The following definition table conventions are implemented in the LaBar Lab: A definition table should exist for every data table that does not hold standardized items (e.g. PANAS, ERQ) that could be looked up in a resource (but a definition table is still recommended). Definition tables should not be used to hold item interpretations, e.g. storing what is meant when tbl_panas.item_panas = 1 (the text of the PANAS’s first item). These belong in a reference table. The table uses the same name as the data table, but replacing ‘tbl_’ with ‘def_’. The table contains the column col_name, which itself only contains the names of columns from the data table. All columns not involved in ID from the data table are specified in col_name. The table contains the column col_desc, which holds a human-readable description of the column. Additional columns can be added using the col_ format, when required. The definition table def_emofreq can now be populated with text: insert into def_emofreq (col_name, col_desc) values (&#39;resp_emofreq_frq&#39;, &#39;Participant response to frequency prompt&#39;), (&#39;resp_emofreq_int&#39;, &#39;Participant response to intensity prompt&#39;), (&#39;resp_emofreq_sal&#39;, &#39;Participant response to salience prompt&#39;); 7.5 Insert Data Building on our example for building databases and tables, now that a database and tables are properly specified we can begin adding data to the tables. Adding data is easily accomplished via the insert into command, and we have seen a few examples already when building the reference and definition tables. To add a single row (or several) of data into a table, we specify the table, columns, and values: use db_test; insert ignore into tbl_erq (subj_id, sess_id, item_erq, resp_erq) values (1, 1, 1, 6), (1, 2, 1, 4), (1, 3, 1, 2); Working within our example database db_test we have added three new rows of data to tbl_erq. The insert ignore into skips the insertion if the same values already exist, avoiding overwriting existing data in tbl_erq. The relevant columns were referenced, and then participant 1 (ref ref_subj.subj_name) responses to the first ERQ item were added for each session. We can check that the insertion was successful by selecting our data: mysql&gt; select * from tbl_erq; +---------+---------+----------+----------+ | subj_id | sess_id | item_erq | resp_erq | +---------+---------+----------+----------+ | 1 | 1 | 1 | 6 | | 1 | 2 | 1 | 4 | | 1 | 3 | 1 | 2 | +---------+---------+----------+----------+ 3 rows in set (0.01 sec) Obviously, manually entering every row is untenable and error prone. The insert command of single rows was shown for practice and also to be leveraged algorithmically in a workflow. If CSV files (or similar formats) already exist in the same format as your table, then entire tables can be inserted as well. The following shell syntax can be used to generate some random data for tbl_erq (and should be modified to build input data for ref_subj, tbl_panas, and tbl_emofreq): #!/bin/bash echo &quot;subj_id,sess_id,item_erq,resp_erq&quot; &gt;${HOME}/erq_data.csv for subj in {1..9}; do for sess in {1..3}; do for item in {1..10}; do resp=$(((RANDOM % 10) + 1)) echo &quot;${subj},${sess},${item},${resp}&quot; &gt;&gt;${HOME}/erq_data.csv done done done Having built properly formatted CSV files for tbl_panas, tbl_erq, and tbl_emofreq, and assuming that local infiles are allowed, login to the server using the local-infile option. Using this options allows the server to read a file on the local machine. $mysql --local-infile=1 -u &lt;user&gt; -p db_test Entire CSVs from the Desktop can now be loaded via the load data command (and note that rows will be skipped for subj_id values not found in ref_subj): delete from tbl_erq; --clear example inputs load data local infile &#39;/path/erq_data.csv&#39; into table tbl_erq fields terminated by &#39;,&#39; enclosed by &#39;&quot;&#39; lines terminated by &#39;\\n&#39; ignore 1 rows; As specified, local infile supplies the path to the CSV file. Then, extra options are specified to help parse the file; such as specifying that the CSV is comma-delimited, strings are enclosed by a double quote, line endings use a linefeed, and a header exists in the CSV. Note: the infile string will require editing to point to the erq_data.csv on your local machine. This method is faster than, and preferred over, manually inserting each row. We can again verify that the table was updated, but limit the return to 10 rows: mysql&gt; select * from tbl_erq limit 10; +---------+---------+----------+----------+ | subj_id | sess_id | item_erq | resp_erq | +---------+---------+----------+----------+ | 1 | 1 | 1 | 6 | | 1 | 1 | 2 | 3 | | 1 | 1 | 3 | 6 | | 1 | 1 | 4 | 1 | | 1 | 1 | 5 | 3 | | 1 | 1 | 6 | 1 | | 1 | 1 | 7 | 5 | | 1 | 1 | 8 | 5 | | 1 | 1 | 9 | 3 | | 1 | 1 | 10 | 5 | +---------+---------+----------+----------+ 10 rows in set (0.00 sec) 7.6 Select Data Data from tables is accessible with the select command. Simply, the anatomy of this command is to specify which columns from which tables are desired (and using an ’*’ when all columns are desired). This section will provide examples of how to extract data from tables using select and join. 7.6.1 Basic Usage After logging into the server and activating the appropriate database ($mysql -u &lt;user&gt; -p db_test), a list of available tables is accessible via show, and describe can be used to see the number, name, and type of columns: mysql&gt; show tables; +-------------------+ | Tables_in_db_test | +-------------------+ | def_emofreq | | ref_emo | | ref_sess | | ref_subj | | tbl_emofreq | | tbl_erq | | tbl_panas | +-------------------+ 7 rows in set (0.00 sec) mysql&gt; describe tbl_erq; +----------+------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +----------+------+------+-----+---------+-------+ | subj_id | int | NO | PRI | NULL | | | sess_id | int | NO | PRI | NULL | | | item_erq | int | NO | PRI | NULL | | | resp_erq | int | YES | | NULL | | +----------+------+------+-----+---------+-------+ 4 rows in set (0.00 sec) Accordingly, we can select all columns of tbl_erq (and limit the selection to the first 5 rows) with select: mysql&gt; select * from tbl_erq limit 5; +---------+---------+----------+----------+ | subj_id | sess_id | item_erq | resp_erq | +---------+---------+----------+----------+ | 1 | 1 | 1 | 7 | | 1 | 1 | 2 | 7 | | 1 | 1 | 3 | 10 | | 1 | 1 | 4 | 1 | | 1 | 1 | 5 | 6 | +---------+---------+----------+----------+ 5 rows in set (0.00 sec) We can also count the number of rows (select count(*) ...) as well as how many entries a specific subject has (select count(*) ... where subj_id = 1): mysql&gt; select count(*) from tbl_erq; +----------+ | count(*) | +----------+ | 270 | +----------+ 1 row in set (0.01 sec) mysql&gt; select count(*) from tbl_erq where subj_id = 1; +----------+ | count(*) | +----------+ | 30 | +----------+ 1 row in set (0.00 sec) Finally, we can select only desired columns (and rows of a specific session ID): mysql&gt; select subj_id, item_erq, resp_erq -&gt; from tbl_erq -&gt; where sess_id = 1 -&gt; limit 5; +---------+----------+----------+ | subj_id | item_erq | resp_erq | +---------+----------+----------+ | 1 | 1 | 7 | | 1 | 2 | 7 | | 1 | 3 | 10 | | 1 | 4 | 1 | | 1 | 5 | 6 | +---------+----------+----------+ 5 rows in set (0.00 sec) 7.6.2 Join Tables 7.6.2.1 Accessing Reference Tables Our example database has a number of tables containing different information. Specifically, the reference tables contain values that are expected to appear across multiple tables while the data tables hold actual participant responses and information, which themselves are linked to the reference tables via foreign keys. Previously, we selected data from tbl_erq but did not interpret the subj_id or sess_id columns, whose values exist in ref_subj and ref_sess, respectively. Combining information from multiple tables is accomplished with the join command. This join is appended to the select command to modify what is returned from the query: select ref_subj.subj_name, ref_sess.sess_name, tbl_erq.item_erq, tbl_erq.resp_erq from tbl_erq join ref_subj on ref_subj.subj_id = tbl_erq.subj_id join ref_sess on ref_sess.sess_id = tbl_erq.sess_id where tbl_erq.sess_id = 2 limit 5; Here, the values (names) of tbl_erq.subj_id and tbl_erq.sess_id are interpreted and included in the output by joining the ref_subj and ref_sess tables. First, columns from three different tables are selected: the name columns from the relevant reference tables as well as the item and resp columns from tbl_erq. Next, the table containing the relevant data is specified (from tbl_erq), and then tbl_erq is joined with the reference tables by specifying which columns will match between the data and reference tables. Specifically, the query is instructed to join ref_subj to tbl_erq, and to match the column values in ref_subj.subj_id and tbl_erq.subj_id to guide the join (and the same for ref_sess) in order to return the specified columns. This join functionality is the bread-and-butter of RDBMS and the reason for which the conventions for both reference and data tables (above) have been specified. Table (and database) names can get large and unwieldy, making for overly verbose select commands. Below is an equivalent command which also assigns the table names to separate variables (a, b, and c): select a.subj_name, b.sess_name, c.item_erq, c.resp_erq from tbl_erq c join ref_subj a on a.subj_id = c.subj_id join ref_sess b on b.sess_id = c.sess_id where c.sess_id = 2 limit 5; Given this selection, we expect the query to return a matrix containing the subject name, session name, and ERQ item and participant responses limited to the second session (and only the first 5 rows). +-----------+-----------+----------+----------+ | subj_name | sess_name | item_erq | resp_erq | +-----------+-----------+----------+----------+ | SUB1 | day2 | 1 | 1 | | SUB1 | day2 | 2 | 6 | | SUB1 | day2 | 3 | 2 | | SUB1 | day2 | 4 | 6 | | SUB1 | day2 | 5 | 7 | +-----------+-----------+----------+----------+ 5 rows in set (0.00 sec) Admin Note: this example assumes sql_mode=only_full_group_by. 7.6.2.2 Joining Data Tables Conducting analyses usually involve combining data across multiple measures, perhaps to investigate the relationship of X and Y or to see if controlling for Z has any impact on an X-Y relationship. In our example, participant responses to different measures (e.g. ERQ, PANAS) are kept in separate data tables (tbl_erq and tbl_panas, respectively), and these data can be aggregated into a single matrix with join. Using simulated data (for syntax, see insert data), the tbl_erq has 10 items, and tbl_panas 20, that were administered to 9 participants for each of 3 sessions: mysql&gt; select distinct(item_erq) from tbl_erq; +----------+ | item_erq | +----------+ | 1 | | 2 | .. | 9 | | 10 | +----------+ 10 rows in set (0.00 sec) mysql&gt; select count(*) from tbl_erq; +----------+ | count(*) | +----------+ | 270 | +----------+ 1 row in set (0.00 sec) mysql&gt; select distinct(item_panas) from tbl_panas; +------------+ | item_panas | +------------+ | 1 | | 2 | .. | 19 | | 20 | +------------+ 20 rows in set (0.00 sec) mysql&gt; select count(*) from tbl_panas; +----------+ | count(*) | +----------+ | 540 | +----------+ 1 row in set (0.01 sec) We can join our reference, ERQ, and PANAS tables, then, using an extension of the join example above: -- Long format select a.subj_name, b.sess_name, c.item_erq, c.resp_erq, d.item_panas, d.resp_panas from tbl_erq c join ref_subj a on a.subj_id = c.subj_id join ref_sess b on b.sess_id = c.sess_id join tbl_panas d on d.subj_id = c.subj_id and d.sess_id = c.sess_id where c.subj_id = 1 and c.sess_id = 1 limit 15 ; As before, values from the reference tables are incorporated into output matrix, and this time we add a join of tbl_erq with tbl_panas using both subj_id and sess_id. Additionally, the output is limited to rows for the first subject and session ID. This output is long formatted, and accordingly we can see that each ERQ response is reported for every PANAS item: +-----------+-----------+----------+----------+------------+------------+ | subj_name | sess_name | item_erq | resp_erq | item_panas | resp_panas | +-----------+-----------+----------+----------+------------+------------+ | SUB1 | day1 | 10 | 5 | 1 | 6 | | SUB1 | day1 | 9 | 5 | 1 | 6 | | SUB1 | day1 | 8 | 2 | 1 | 6 | | SUB1 | day1 | 7 | 5 | 1 | 6 | | SUB1 | day1 | 6 | 9 | 1 | 6 | | SUB1 | day1 | 5 | 6 | 1 | 6 | | SUB1 | day1 | 4 | 1 | 1 | 6 | | SUB1 | day1 | 3 | 10 | 1 | 6 | | SUB1 | day1 | 2 | 7 | 1 | 6 | | SUB1 | day1 | 1 | 7 | 1 | 6 | | SUB1 | day1 | 10 | 5 | 2 | 5 | | SUB1 | day1 | 9 | 5 | 2 | 5 | | SUB1 | day1 | 8 | 2 | 2 | 5 | | SUB1 | day1 | 7 | 5 | 2 | 5 | | SUB1 | day1 | 6 | 9 | 2 | 5 | +-----------+-----------+----------+----------+------------+------------+ 15 rows in set (0.00 sec) While perfectly acceptable, the output matrix has a lot of redundant information. One solution is to craft an output matrix in tidy format. That is, rather than pulling both tbl_erq.item_erq and tbl_panas.item_panas, as they are both integer, we could instead create a column titled item_num that has the full range of items. This would allow us to join the response columns, tbl_erq.resp_erq and tbl_panas.resp_panas by their item values. Caution is warranted, however, given the nature of the default join performance (inner) since ERQ and PANAS have a different number of items. Briefly, an ‘inner’ join returns the requested values from where there is a match in both tables while a ‘left’ join returns all records from the first table even if no matching record is found in the second (see here for some helpful diagrams). Accordingly, we will use tbl_panas.item_panas to create the new item_num column and a left join: -- Tidy format select a.subj_name, b.sess_name, d.item_panas as item_num, c.resp_erq, d.resp_panas from tbl_panas d join ref_subj a on a.subj_id = d.subj_id join ref_sess b on b.sess_id = d.sess_id left join tbl_erq c on c.subj_id = d.subj_id and c.sess_id = d.sess_id and c.item_erq = d.item_panas where d.sess_id = 1 limit 22 ; The output tidy matrix, then, does not have the repeating response values found in the long format but NULL values are instead filled for resp_erq when a corresponding item value does not exist. +-----------+-----------+----------+----------+------------+ | subj_name | sess_name | item_num | resp_erq | resp_panas | +-----------+-----------+----------+----------+------------+ | SUB1 | day1 | 1 | 7 | 6 | | SUB1 | day1 | 2 | 7 | 5 | | SUB1 | day1 | 3 | 10 | 7 | | SUB1 | day1 | 4 | 1 | 5 | | SUB1 | day1 | 5 | 6 | 2 | | SUB1 | day1 | 6 | 9 | 7 | | SUB1 | day1 | 7 | 5 | 5 | | SUB1 | day1 | 8 | 2 | 10 | | SUB1 | day1 | 9 | 5 | 10 | | SUB1 | day1 | 10 | 5 | 3 | | SUB1 | day1 | 11 | NULL | 1 | | SUB1 | day1 | 12 | NULL | 3 | | SUB1 | day1 | 13 | NULL | 2 | | SUB1 | day1 | 14 | NULL | 2 | | SUB1 | day1 | 15 | NULL | 7 | | SUB1 | day1 | 16 | NULL | 9 | | SUB1 | day1 | 17 | NULL | 1 | | SUB1 | day1 | 18 | NULL | 1 | | SUB1 | day1 | 19 | NULL | 7 | | SUB1 | day1 | 20 | NULL | 10 | | SUB2 | day1 | 1 | 4 | 9 | | SUB2 | day1 | 2 | 10 | 8 | +-----------+-----------+----------+----------+------------+ 22 rows in set (0.00 sec) 7.7 Python and MySQL Python can be used to connect to the MySQL server both locally (i.e. on labarserv2) and remotely (e.g. from the Duke Compute Cluster). A number of packages are available to support this, but here we will focus on mysql-connector-python (also, here) and pymysql as they seem to work best with our compute environments at the time of this writing. While everything Python does can be done manually when working on the MySQL server directly, we can leverage Python to automate updating, managing, and downloading from the databases. 7.7.1 Connecting Locally To interact with a MySQL database locally, start by installing mysql-connector-python into your python environment: pip install mysql-connector-python Next, we can start a python file, import mysql.connector, and establish a connection between Python and the MySQL server. This is done by creating a connect object using mysql.connector. When establishing the connection, the user, host, password, and database name are specified. For the user nate and password of foobar, we would then create the connection object with the db_test database via: import mysql.connector db_con = mysql.connector.connect( host=&quot;localhost&quot;, user=&quot;nate&quot;, password=&quot;foobar&quot;, database=&quot;db_test&quot; ) It is bad practice to use actual passwords in your code, particularly if you intend to track changes with Git and/or make your repository public. A better option would be to store your password somewhere and then access it through the operating system. Here, I have my password accessible from the variable $SQL_PASS: import os import mysql.connector db_con = mysql.connector.connect( host=&quot;localhost&quot;, user=&quot;nate&quot;, password=os.environ[&quot;SQL_PASS&quot;], database=&quot;db_test&quot;, ) With a connection established, we next create a cursor object. This cursor object is used to interact with the MySQL server, allowing for the execution of statements or commands, which is to say that the cursor object has a number of methods available. The cursor is available as an attribute of the connection object: db_cur = db_con.cursor() 7.7.2 Connecting Remotely Given that the MySQL server exists on labarserv2, it may not always be feasible to login to labarserv2 to execute SQL queries due to time or resource constraints. For instance, a complex analysis could be conducted on the Duke Compute Cluster (DCC) that require interacting with databases on labarserv2, and manually moving CSVs between servers is ill-advised and inefficient. Practically speaking, it is more common to interact with a SQL database over the network (remotely), such is the case when a researcher is working on their personal/work machine with data that exists on a remote server. Here we will describe setting up an SSH Tunnel that will allow us to connect to the remote server. We will use a number of packages: pymysql (which works better than mysql-connector-python for the DCC-labarserv2 connection for whatever reason), paramiko, and sshtunnel. To start, install the required python packages via pip or conda: pip install pymysql pip install paramiko pip install sshtunnel Next, we need to make sure that we have an RSA key set up on our local machine for the remote (or on the DCC for labarserv2). Additionally, we should store our MySQL password (for the account which exists on the remote server) in our operating system. I have my RSA key to labarserv2 accessible via $RSA_LS2, and my MySQL password via $SQL_PASS. Starting a new python file, we can import the relevant packages and then access the RSA key with paramiko: import os import pymysql import paramiko import sshtunnel as SSHTunnelForwarder rsa_ls2 = paramiko.RSAKey.from_private_key_file(os.environ[&quot;RSA_LS2&quot;]) Next, we can set up for the SSH Tunnel. The remote IP of labarserv2 is ccn-labarserv2.vm.duke.edu, my user name for labarserv2 nmm51, and we will bind the default MySQL port 3306 to 127.0.0.1: ssh_tunnel = SSHTunnelForwarder( (&quot;ccn-labarserv2.vm.duke.edu&quot;, 22), ssh_username=&quot;nmm51&quot;, ssh_pkey=rsa_ls2, remote_bind_address=(&quot;127.0.0.1&quot;, 3306), ) Finally, the tunnel is started: ssh_tunnel.start() With an SSH tunnel established, we can then establish a connection to the MySQL database as we did above, referencing the host and newly established tunnel/port. db_con = pymysql.connect( host=&quot;127.0.0.1&quot;, user=&quot;nmm51&quot;, passwd=os.environ[&quot;SQL_PASS&quot;], db=&quot;db_test&quot;, port=ssh_tunnel.local_bind_port, ) Note (a) that the options for specifying the password and database differ from above (which used mysql.connector.connect), and (b) such a 'user'@'host' account will have to exist in the MySQL server, in this case 'nmm51'@'127.0.0.1'. A cursor can then be established in the same way as above: db_cur = db_con.cursor() 7.7.3 Inserting with Python With a connection and cursor established, we are ready to setup some syntax to execute on the server. In this example, we have a CSV available of ERQ responses formatted for tbl_erq available at ${HOME}/erq_data.csv (see Insert Data). This data can be loaded as dataframe: import os import pandas as pd df_erq = pd.read_csv(os.path.join(os.environ[&quot;HOME&quot;], &quot;erq_data.csv&quot;)) Inserting data into tables can be accomplished by the cursor’s execute method, which requires two arguments – the operation/command and a tuple of data. Accordingly, we ready for data insertion by building the first argument, a string that contains SQL syntax and utilizes the MySQL variable %s in the values field rather than actual data: in_cmd = ( &quot;insert into tbl_erq &quot; + &quot;(subj_id, sess_id, item_erq, resp_erq) &quot; + &quot;values (%s, %s, %s, %s)&quot; ) Next we can extract the first row of df_erq as a tuple to create the second execute argument. MySQL does not support int64 types, the default integer value in df_erq, so we will convert the values to int when creating the tuple. in_val = tuple([int(x) for x in df_erq.values[0]]) With both execute arguments specified, we can insert the data into tbl_erq by triggering the cursor’s execute method, followed by the connection’s commit method. Note: if following this tutorial, it might be useful to login to the server and delete the tbl_erq data which was previously inserted in order to verify that these commands are working (mysql&gt; delete from tbl_erq;). db_cur.execute(in_cmd, in_val) db_con.commit() With this basic insertion method working, we could then loop through pandas dataframes to build or update tables. Here we will add all contents of df_erq to tbl_erq, row-by-row (and also change the df_erq column types to simply the tuple generation): df_erq = df_erq.astype(&quot;object&quot;) for idx in df_erq.index: in_val = tuple(df_erq.values[idx]) db_cur.execute(in_cmd, in_val) db_con.commit() While working with each row is fast enough for small datasets, this quickly becomes (extremely) inefficient when datasets are larger and accordingly a row-wise approach is not the recommended. A much better way to upload an entire dataset is with the cursor’s executemany method. Similar to execute, executemany takes two arguments – the operation and a list of tuples. In preparation, then, we can create a list of tuples from our dataframe via pandas’ itertuples: in_val = list( df_erq[[&quot;subj_id&quot;, &quot;sess_id&quot;, &quot;item_erq&quot;, &quot;resp_erq&quot;]].itertuples( index=False, name=None ) ) Lastly, we upload the entire dataset via executemany: db_cur.executemany(in_cmd, in_val) db_con.commit() 7.7.4 Selecting with Python To retrieve data from our database tables, we can begin specifying SQL commands for execution once a connection and cursor have been established. We’ll start by selecting the first 10 rows of tbl_erq. As with inserting data, we will use the cursor’s execute method, but this time only a single argument containin SQL syntax will be required: query = &quot;select * from tbl_erq limit 10&quot; db_cur.execute(query) With the execution complete, we are able to fetch results from the query. The fetchall method returns a list of tuples, which themselves can be used to build a pandas dataframe: import pandas as pd result = db_cur.fetchall() df_erq = pd.DataFrame( result, columns=[&quot;subj_id&quot;, &quot;sess_id&quot;, &quot;item_erq&quot;, &quot;resp_erq&quot;] ) A more useful select command will likely involve joining across tables. For instance, perhaps we are interested in all day2 data from tbl_erq (limited to the first 5 rows): query = &quot;&quot;&quot;select a.subj_name, b.sess_name, c.item_erq, c.resp_erq from tbl_erq c join ref_subj a on a.subj_id = c.subj_id join ref_sess b on b.sess_id = c.sess_id where c.sess_id = 2 limit 5; &quot;&quot;&quot; db_cur.execute(query) df_join = pd.DataFrame( db_cur.fetchall(), columns=[&quot;subj_name&quot;, &quot;sess_name&quot;, &quot;item_erq&quot;, &quot;resp_erq&quot;], ) It is likely useful to note that python variables can be included in the query string. Here we select all data from a single subject by their ID: subj_id = 1 query = f&quot;select * from tbl_erq where subj_id = {subj_id}&quot; db_cur.execute(query) df_subj = pd.DataFrame( db_cur.fetchall(), columns=[&quot;subj_id&quot;, &quot;sess_id&quot;, &quot;item_erq&quot;, &quot;resp_erq&quot;], ) 7.7.5 Closing Connections To avoid crashing the MySQL server, it is import to close the connection with the server once all requests are finished. This can be done as simply as using the close method: db_con.close() Another option is to utilize Python’s with syntax to aid in connection closing: with mysql.connector.connect( host=&quot;localhost&quot;, user=&quot;nate&quot;, password=os.environ[&quot;SQL_PASS&quot;], database=&quot;db_test&quot;, ) as db_con: db_cur = db_con.cursor() db_cur.execute(&quot;select * from tbl_erq limit 10&quot;) df_erq = pd.DataFrame( db_cur.fetchall(), columns=[&quot;subj_id&quot;, &quot;sess_id&quot;, &quot;item_erq&quot;, &quot;resp_erq&quot;], ) Likewise, SSH tunnels need to be closed at the end of the process when working remotely: ssh_tunnel.stop() 7.8 R and MySQL R can be used to connect to the MySQL server both locally (i.e. on labarserv2) and remotely (e.g. from a work station). While a number of packages exist that can manage the connection between the R and MySQL servers, here we will focus on DBI and RMySQL. DBI allows for communication between R and an RDBMS and supplies many of the functions we will use to interact with the database and tables. RMySQL facilitates an interface directly with MySQL (see here for a quick usage reference). Start by installing RMySQL: install.packages(&quot;RMySQL&quot;) RMySQL should also install DBI as a dependency, but if it does not then we can make sure we have DBI: install.packages(&quot;DBI&quot;) 7.8.1 Connecting Locally To interact with a MySQL database locally, we first start by establishing a connection between R and MySQL. Start a new R file, load the RMySQL library, and setup a connection via dbConnect that specifies we are using the MySQL driver: library(&quot;RMySQL&quot;) db_con &lt;- dbConnect( RMySQL::MySQL(), dbname = &quot;db_test&quot;, host = &quot;localhost&quot;, port = 3306, user = &quot;nate&quot;, password = Sys.getenv(&quot;SQL_PASS&quot;) ) Here, I specify the desired database via the dbname option, and used default host and port arguments. Additionally, as it is bad practice to embed passwords in code (particularly when tracking changes with Git or when taking the code public!) I have my MySQL password saved in the .RProfile and available via SQL_PASS. Finally, we can check our connection to the database by listing tables via dbListTables: dbListTables(db_con) 7.8.2 Connecting Remotely Given that the MySQL server exists on labarserv2, it may not always be feasible to login to labarserv2 to execute SQL queries due to time or resource constraints. Commonly, analysis scripts exist locally on a work station and the researcher aims to download data for local analyses. One approach is to setup an SSH Tunnel between our local machine and labarserv2: $ssh -i $RSA_LS2 \\ nmm51@ccn-labarserv2.vm.duke.edu \\ -L 3308:127.0.0.1:3306 \\ -N Here, I have specified an SSH connection and supplied a number of options. Specifically, I supply my RSA key for labarserv2 (available in the OS via $RSA_LS2) with the -i option, I bind my local port 3308 with the default MySQL IP of 127.0.0.1 and MySQL port of 3306 via the -L option. Finally, the -N option is useful for port forwarding as a remote command is not executed. This ssh command will cause the terminal to hang, and I can also check that the port is open via $ps aux: $ps aux | grep ssh | grep 3308 Nathan 195108 0.0 0.0 14708 6404 pts/4 S+ 12:49 0:00 ssh -i /home/Nathan/.ssh/id_rsa_labarser2 nmm51@ccn-labarserv2.vm.duke.edu -L 3308:127.0.0.1:3306 -N With an SSH tunnel established, we can then create a connection between R and MySQL using a nearly identical command as above, save only that we specify our local port and the MyServer IP: db_con &lt;- dbConnect( RMySQL::MySQL(), dbname = &quot;db_test&quot;, host = &quot;127.0.0.1&quot;, port = 3308, user = &quot;nate&quot;, password = Sys.getenv(&quot;SQL_PASS&quot;) ) Finally, we can check our connection to the database by listing tables via dbListTables: dbListTables(db_con) 7.8.3 Inserting with R With a connection between R and MySQL established, we can begin to interact with the database. In this example, we have a CSV available of ERQ responses formatted for tbl_erq available at ${HOME}/erq_data.csv (see Insert Data). This data can be loaded as a dataframe: df_erq &lt;- read.csv(paste(Sys.getenv(&quot;HOME&quot;), &quot;erq_data.csv&quot;, sep = &quot;/&quot;)) One method of inserting data into a table is by building an insert object with sqlInterpolate and then passing this object to dbSendQuery. The function sqlInterpolate takes three arguments: the connection object, a SQL command, and data. As we have the connection object saved as db_con (above), we can next specify a SQL insert command that uses ?val to identify the variable val: in_cmd &lt;- &quot;insert into tbl_erq (subj_id, sess_id, item_erq, resp_erq) values (?val1, ?val2, ?val3, ?val4)&quot; Here, four variables are set (val1-4) to serve as placeholders for data. To specify data (the third argument of sqlInterpolate), we create a named list which uses the SQL variables from the insert command (val1-4). Doing this for a single row of data (df_erq[1, ]), we can assign each df_erq column to their respective variable: in_val &lt;- c( val1 = df_erq[1, 1], val2 = df_erq[1, 2], val3 = df_erq[1, 3], val4 = df_erq[1, 4] ) Finally we build our insert object with sqlInterpolate, and pass that object to dbSendQuery. The function of dbSendQuery is to submit and the execute whatever statement is supplied as the second argument, here our insert object. insert &lt;- sqlInterpolate(db_con, in_cmd, .dots = in_val) dbSendQuery(db_con, insert) With this simple insertion method working, we can loop through the dataframe to iteratively insert data into tbl_erq. Here we submit data row-by-row: for (row in 1:nrow(df_erq)) { in_val &lt;- c( val1 = df_erq[row, 1], val2 = df_erq[row, 2], val3 = df_erq[row, 3], val4 = df_erq[row, 4] ) insert &lt;- sqlInterpolate(db_con, in_cmd, .dots = in_val) dbSendQuery(db_con, insert) } A few words of caution – first this insertion method will always append the table, which can potentially result in rows that contain identical data (and potentially corrupt a database). Second, while fast enough for illustrative purposes, inserting each row quickly becomes extremely inefficient as the size of the dataframe increases. We can overcome these issues by utilizing dbWriteTable instead of sqlInterpolate and dbSendQuery , which has the added benefit of a simplified specification. For instance, we can add the first row of df_erq to the table tbl_erq via : dbWriteTable(db_con, &quot;tbl_erq&quot;, df_erq[1, ], row.names = FALSE) If this data already existed in the table then this command would return an error rather than simply adding an identical row of data to the table. Additionally, to resolve or avoid these errors, dbWriteTable has append and overwrite options that can be selected. Finally, and perhaps more usefully, entire dataframes can be written to the tbl_erq table: dbWriteTable(db_con, &quot;tbl_erq&quot;, df_erq, overwrite = TRUE, row.names = FALSE) 7.8.4 Selecting with R To retrieve from the database, we can utilize dbSendQuery which requires two arguments: the connection object (which we have as db_con) and a SQL statement. Note that while dbSendQuery submits and executes commands on the SQL server (as we saw above), it does not extract data and accordingly we save the object. query &lt;- dbSendQuery(db_con, &quot;select * from tbl_erq&quot;) The output matrix of the select command can then be loaded into R via dbFetch: df_erq &lt;- dbFetch(query, n = -1) dbClearResult(query) All records selected in the query are retrieved into df_erq by specifying n = -1, and we are required to close this query via dbClearResult whenever some object is obtained from dbSendQuery (i.e. we do not have to dbClearResult when using dbWriteTable). When working in larger workflows and with complicated selection commands, it might be useful to save SQL syntax as its own file and then read said file when building the query object. For instance, the following statement joins the tbl_erq and tbl_panas, while also converting from long to tidy format: select a.subj_name, b.sess_name, d.item_panas as item_num, c.resp_erq, d.resp_panas from tbl_panas d join ref_subj a on a.subj_id = d.subj_id join ref_sess b on b.sess_id = d.sess_id left join tbl_erq c on c.subj_id = d.subj_id and c.sess_id = d.sess_id and c.item_erq = d.item_panas where d.sess_id = 1 limit 22 ; This SQL file (saved at ${HOME}/dl_erq.sql) can then be read by the readr package, and incorporated into the dbSendQuery command: sel_erq &lt;- paste(Sys.getenv(&quot;HOME&quot;), &quot;dl_erq.sql&quot;, sep = &quot;/&quot;) query &lt;- dbSendQuery(db_con, statement = readr::read_file(sel_erq)) df_join &lt;- dbFetch(query, n = -1) dbClearResult(query) 7.8.5 Closing Connections Once interacting with the MySQL database is no longer necessary, it is important to close the connection in order to avoid crashing the server. This is easily accomplished with dbDisconnect: dbDisconnect(db_con) Likewise, an SSH Tunnel can be closed with ctl-C or by terminating the process via pkill. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
